In Debian:

apt list <packagename> 	(searches for packages with <packagename> in the name)(-a for all matching packages, not just first match)

apt search <name> 	(searches for packages not only maching packagename, but also the description)

apt show <packagename>	(to see more info about the package, including dependencies)(-a option also works here)

sudo apt remove <packagename>

apt list --installed | less

sudo dpkg-reconfigure <packagename>	(shows you/manipulates config options for installed packages)

--------

sudo nmcli c d "Wired Connection 1"	(c to show connection[s], d for 'down' [to disable] or u for up [to enable])

nmcli c (show connections)
nmcli d (show devices)

sudo nmcli c e "Wired Connection 1"	(e to edit the connection)

-------

sudo zcat /var/log/syslog.2.gz	(to view old compressed logs)

sudo vim /etc/rsyslog.conf	(to view/configure rsyslog's behavior)

Other programs may write to the systemlog, or create their own files in the log directory.

sudo tail /var/log/auth.log	(for auth info, or kern.log for kernel info)

sudo dmesg	(instead of /var/log/kern.log to view kernel info)

------------------------------------------------^----Debian Done.

In Ubuntu 18.04:

apt-get download <packagename>		(to only download the package as a deb file instead of installing it)
sudo dpkg -i <downloadedpackagename>	(to install the downloaded package from a .deb file)
sudo dpkg -r <packagename>		(to remove a package using dpk)

-------

At the terminal prompt, $ suggests you are logged in as a regular/standard user, while # means the root account.

Ctrl+Alt+{F2-F6} to access the tty terminals. Type 'logout' to logout.

passwd (to change password)

Ctrl + c (end a running program and return to prompt)
Ctrl + d (logout of the current shell session, is the same as typing 'exit' or 'logout')
Ctrl + h (generate a backspace character)
Shift + pageup/pagedown	(browse the terminal buffer)

yelp info:info	(brings up a graphical help screen [like a pdf])(from the info application, the info section)
yelp man:ls
yelp will not find shell builtin commands.

---------

Special Files are the mechanism used for input and output. Most special files are in the /dev directory.
Domain|Sockets is a special filetype, providing inter-process networking protected by the filesystem's access control.
Named Pipes act more or less like sockets and form a way for processes to communicate with each other, without using network socket semantics.

ls -F	(suffixes filenames with characters like /,=,*,@ to indicate the file type)
df -h .	(the '.' is for current directory, to find info about which partition the current directory is on)

echo $PATH
echo PATH=/usr/local/sbin:usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/john
	(is temporary, /home/john was just added by the user temporarily)
echo PATH=$PATH:/home/john	(shorthand) (to make this permanent, add this line to the end of the .bashrc file)
source ~/.bashrc (or .profile) (to apply the change without needing to log out of the terminal)

cat /etc/shells	(shows valid login shells)
/bin/sh is usually a link to bash which will execute a bourne-compatible shell when called in this way.

/etc/passwd is where your default shell is set.

To switch from one shell to another, just type in the name of the shell in the terminal prompt, i.e. sh.
echo $SHELL	(a way to check which shell you are currently using)
echo $HOME	(shows the location of your home directory)

/etc/bash.bashrc	(systemwide .bashrc file)(To enable the settings / commands in this file for login shells as well, this file has to be sourced in /etc/profile.)

----------

ls -t	(sorted by modification date, newest first) (ls -tr for oldest first [-r for reversing the order])
ls -S	(sorted by size, biggest first) (ls -Sr for smallest first)
ls -ld	(to list directories only)(for some reason, did NOT WORK for me on MX Linux)

On most linux versions, ls is aliased to color-ls by default.

alias ls	(show what ls [or any given command] is aliased to/as)

----------

find path -name 'search string'
find -name '*' -size +1000k	(searches all subdirectories beginning at current directory)
find -name '*' -size +1000M	(capital M for megabytes)
find . -name '*.tmp' -exec rm {} \;

slocate	(a security enhancement of the 'locate' command which prevents users from getting output they have no right to read)

---------

ln -s targetpath/file newlycreatedshortcut

---------

Use a trailing & to put a job in the background so that the terminal is still usable while the process is running.

Automatic processes: 'at' command for executing at a certain date and time, 'batch' command for executing whenever the total system load is low enough to accept extra jobs (by default, when system load is less than 0.8) (the service 'atd' or 'atrun' should be running to perform the actual execution of the commands set by batch).

--------

tail -10 .bash_history

filtering means using the pipe |

ps -af	(shows only most frequently called processes opened by the current user)
ps -ef | grep mgtow1	(all processes owned by the user mgtow1)

pstree	(tree view of processes)

The first line of the 'top' command shows the same information displayed by the 'uptime' command.

-------

nice -n <nice_value> <process_name>

renice -n <nice_value> -p <PID>

In 'top', you can press 'r' then the PID to renice a process. Also, 'k' to kill a process.

top -d 10	(to delay the updates to a 10-second interval, makes it easier to track a specific process)

xkill	(allows you to click on any application you want to kill. Is dangerous because it uses a SIGKILL by default, so only use it when an application hangs.)

sleep 1900; echo "lunchtime" &	(shows you "lunchtime" after 1900 seconds)

lp file1; sleep 1000; lp file2; sleep 1000; lp file3	(to allow users to print in between your printing of large files)

at tomorrow + 2 days	(you will be given a prompt to type the desired command to be run, then press 'Ctrl + D' to apply it)
at 1430
at now
The 'at' command will send conformation when a job is done, or an explanation as to why it couldn't be done.

atq	(to list jobs)
atrm 91	(remove job number 91)

-------

REGULAR EXPRESSIONS:

tr, sed, vi and grep commonly use regular expressions.
regexp: .,^,$,*,\,(),?
interval regex: {n},{n,m},{n,}	(respectively, matches preceding character exactly n times, n to m times, or n or more times)
extended regex: \+ (match one or more occurence of the previous character), \? (matches zero or one occurence of the previous character)
brace expansion: {a..z} 	('..' means 'to', so this would output the letters a through z)({0..10} would output the numbers 1 thru 10)

cat file1 | grep -E p\{2}	(will match 'apples' [2 p's in a row])
cat file1 | grep 'a\+t'		(will match lines where 't' is preceded by 'a' [Dates, Kumquat])
echo a{0..10}b	(output a0b a1b a2b a3b....a10b)

--------

cat file1 file2 > file3		(creating a new file like this is called 'truncating')

grep pattern1 file | grep -v pattern2	(-v means inverse, matches pattern1 then eliminates pattern2)

ls -la | grep filename(or part of filename)	(to find files in a directory)

mail john@hotmail.com < file

spell < test.txt > error.log	(spellcheck test.txt and redirect all output to error.log)

ls > list 2>&1	(to output both stdout and stderr to the file 'list')

ls 2>&1 > list (will only output stdout to the file list, because of the > at the end)

<commandname> 2>&1 | less	(to analyze the errors made by a program/command)

tty	(to find out which pseudo terminal you are using)
make 2> /dev/pts/0	(0 or any other terminal number, this sends the error messages to a specific terminal)

date | tee file1 file2	(will both out to the screen [stdout] and to both files)

--------

ftp <Domain_name or IP_Address>	(ftp = file transfer protocol)(Allows you to upload, download and browse files)
	Then it will ask you for username and password to log in, and give you the ftp> prompt. From here, you can use:
		ls (list directories with security settings)
		dir (display files in current directory of remote computer)
		cd, put <filename> (to upload a file from your local computer to the remote computer), get <filename> (download), quit (logout)
ftp transmissions are not encrypted, so anyone can read the data you send, including your username and password, so be careful.

--

telnet <remote_hostname or IP_Address>	(allows remote administration)
	You can now execute shell commands on the remotehost, using the same syntax as you normally would locally. 'logout' to exit.

--

ssh <remote_username>@<remote_hostname or IP_Address>
	Then you can normally execute any shell commands that you can execute in your terminal.
ssh is more secure than telnet, because it uses digital certificates to authenticate, and encrypts passwords.

----------

sudo adduser bob	(will prompt you to fill each field in turn)
sudo passwd -l bob	(to lock bob's account)
sudo userdel -r bob	(to remove the user and also delete his mail spool)

-

groupmod <press Tab key twice for autocomplete to show available groups>
sudo usermod -aG <groupname> <username>	(to add a user to a group)
cat /etc/group | grep <groupname>	(to check which users are in a certain group)
sudo deluser <username> <groupname>	(to remove a user from a group)
groups		(to check which groups you are a member of)

-----------

PERMISSIONS:			r				w				x
	file		open and read a file		modify file's contents		execute an executable file
	directory	list directory's contents	add/remove/rename files in dir	enter a dir and gain possible access to subdirectories

If you have write permission on a file but not its directory, you can modify the file's contents, but will not be able to add,remove or rename any files in the directory.

---

sudo chown [options] <ownername>:<groupname> <filename>
sudo chown [options] <ownername> <filename>

sudo chgrp <groupname> <filename>

Two groups CANNOT own the same file.
We do NOT have nested groups in linux. Meaning that one group CANNOT be a subgroup of another group.

--------------------------------------------------^-------------Ubuntu 18.04 Done.

BASH SCRIPTING:

cd -	(jump to last accessed directory)

pushd '<directoryname>'	(add a directory into the 'stack')
pushd < <directoryname>	(switch to directory and push it onto the stack)
pushd		(switch the first and second directories on the stack)
popd 		(kick the most recently pushed directory out of the stack)

----

cat >> file1.txt	(will prompt you to keep typing [you can also use newlines] until you finish with Ctrl+D)

----

su - fuckwad	(switch user to fuckwad, and using fuckwad's environment)

----

watch free -h	(constantly run 'free -h', update every 2 seconds by default)

killall <programname>	(does SIGTERM by default, can use -s <number> to change it)

exit	(first quits the user you changed to, then closes out of the terminal itself)

----

In terminal:

alt + d	(to delete the rest of the word, or delete a whole word if at the beginning of that word)
alt + t	(to swap the current word with the word before it in the same line i.e. 'Bruce moving the box' becomes 'Bruce the moving box')
ctrl + t	(to swap the current character with the character directly before it on the same line i.e 'corner' becomes 'conrer')
ctrl + _	('underscore') (undoes changes one at a time)
ctrl + w	(to cut the word BEFORE the cursor)
ctrl + y	(to paste the word cut by ctrl + w exactly on the cursor)
alt + u	(to capitalize the rest of the word starting from the cursor position)
alt + l	(to lowercase the rest of the word starting from the cursor position)
alt + c (to capitalize the character the cursor is on then move the cursor to the space at the end of the word)

-----------------

Bash Scripting:

The 'echo' command automatically adds a newline character at the end of each quoted section.
echo -n "Hello World" 	(to eliminate the automatic newline character)
echo "Hello \n World"	(will NOT work, will simply output 'Hello \n World'. To terminate/remove the special characters, use
echo -e "Hello \n World"	('-e' enables interpretation of backslash escapes) (\n is newline, \t is tab)
echo Hello World	(will also work, but the double quotes allows special formatting and special characters to be used, so it is best to use them)
If you have a quotation mark inside the body, you can use \ to escape it (echo "Hello \"World\"")

-----

Comment is #

Multiline comment is colon, space, single quote, text, single quote.

: 'Line1
Line2
Line3' 

----

num1=17
var1="Hello World"	(there must NOT be any spaces around the '=' sign, otherwise it will give a syntax error)
echo var1	(outputs 'var1')
echo $var1	(outputs 'Hello World')
echo "This is "$num1	(IS THE SAME AS)	echo "This is $num1"	(SAME AS)	echo "This is" $num1
	i.e. you can call variables from within the double-quotes or outside of them, and spaces outside and inside the "" are the same

((result=num1/7))	(IS THE SAME AS)	echo $num1/7 | bc
echo $result	(will only give you an integer value)

echo $num1/7 | bc -l	(will give you a floating point number i.e. will use decimal points) (2.42857142857142857142)

---

var1=Hello
var2=World
echo $var1+$var2	(will literally output 'Hello+World') (SAME AS) result=$var1+$var2; echo $result
result="$var1$var2"	(will output 'HelloWorld' without a space)
result="$var1 $var2"	(to do it correctly 'Hello World')

---

var1="1"
var2="2"
var1+=$var2	('+=' joins the strings together)
echo $var1	(will output '12' i.e. literally putting the 1 and 2 together, side by side)
The above also works if vars 1 and 2 are just words, i.e. John Smith = 'JohnSmith')

---

str="Hello World"
sub={$str:6:3}	(start from the 6th character [which would be the space after Hello] and output the next 3 characters after it])
echo $sub	(will output 'Wor')

----

WHILE LOOPS:

count=0
while [ $count -lt 10 ]; do			(-lt is less than, -le is less than or equal to, -ge, -gt) (for <,<= etc use dbl parans (()) )
	echo "Loop is at $count"
	let count=count+1
done

----

FOR LOOPS:

for i in $( ls ); do			(for every word outputted by the 'ls' command,
	echo $i				 output that word) (seperates long filenames into words)
done

---

for i in `seq 1 25`; do			(for every number in 1-25,	(only works in ascending order: `seq 25 1' will output 1-25)
	echo $i				 print each number)
done

----

UNTIL LOOPS:

count2=25
until [ $count2 -lt 10 ]; do		(run the program until the condition x<10 becomes true)
	echo $count2
	let count2-=1			(subtract one each time, like x=x-1)
done					(outputs 25-10) (to stop at 11 instead of 10, do -le x<=10 instead so it stops the program early)

---

BREAK AND CONTINUE:

count=0
while [ $count -lt 10 ]; do
	let count+=1		(same as x=x+1)
	if [ $count -eq 5 ];
	then
		break		(stops and exits the program) (if you use 'continue' instead of break, it will output 1-4,6-10 skipping the '5'
	fi			 because it will skip over the rest of the code and then continue the program from the beginning)
	echo $count
done				(will output 1-4)

----

GET USER INPUT (read variablename):

echo "Enter your favorite shit:"
read shitname
echo "Your favorite shit is $shitname"

----

IF STATEMENTS:

echo "Enter a number"
read num
if [ $num -lt 10 ]				(you can also use and/or i.e. 'if [[ ($num -lt 10 && $num2 -eq 5) ]];)
then							(the && means that BOTH conditions MUST be TRUE to run that section of code)
	echo "Your number $num is less than 10"		(a || would mean that only one condition needs to be true)
fi							(note that to use && or || you must use double [[ and single ( as shown above)

MORE COMPLEX:

echo "Enter a number"
read num
if [ $num -lt 10 ];
then
	echo "Your number $num is less than 10"
elif [ $num -lt 20 ];				(you can use as manu elifs as you want)(elif = else if)
then
	echo "Number is less than 20"
fi

EVEN MORE COMPLEX:

echo "Enter a number"
read num
if [ $num -lt 10 ];
then
	echo "Your number $num is less than 10"
elif [ $num -lt 20 ];				
then
	echo "Number is less than 20"
else						('else' is a catchall for if ANY OTHER CASE happens)
	echo "Number not rebognized"
fi

----

CASE STATEMENTS:

The case statement is basically like an if/elif/else statement, but a simplified version of it.

-

case $num in				(if input is)
1)					('1')
echo "This is 1 with a case";; 		(then run this code)(terminate each branch with ;;)

-

case $num in
1)
echo "This is 1 with a case";;
2)
echo "This is 2 with a case";;
3)
echo "This is 3 with a case";;
*)				(in the case of ANY OTHER value)
echo "Not valid";;
esac				(end the case statement)

----

GETTING ARGUMENTS:

totalArgs=$#
num1=$1
num2=$2			(so arguments are notated by $x, where 'x' is the number of the argument's order)
result=$(($num1*$num2))	(need the double parens for mathematical operations)
echo "Total number of arguments is $totalArgs"
echo "$1 times $2 is $result"

----

FUNCTIONS:

function Hello()		(creating a function)
{
	echo "Put your code here"
}

Hello		(You must call a function/method like this for it to be run)
Hello		(To run it again)

---

Adder()		(you do not need to write 'function' to craete a function)
{
	result=$(($1+$2))		(adds the first two arguments together upon calling the function)
	echo "Result is $result"
}

Adder 8 10		(runs the function as 8 + 10 = 18)
			(Instead of calling it like the above, you can also put it into a variable by doing
			 value=$(Adder 7 6)    where 'value' is just a variable name)(unclear if the prefix 'function' is needed in this case)

----

GLOBAL vs LOCAL VARIABLES:

Global variables are created outside of a function (even within the same bash script, just outside the function itself).
Local variables are created between the two curly braces of a function, and can ONLY be used INSIDE of that function.

----

ARRAYS:

arr=( "Fuckwad" "Shithead" "Cumslut" )			(you can also add numbers to this, without the doublequotes)
size=${#arr[@]}
echo $size		(will output '3' for the number of items in the array)
index=2
value=${arr[${index}]} 
echo $value		(will output 'Cumslut' because it is the third value in the array [starting from 0 for first, 1 for second, 3 for third])

--

for (( i=0; i<$size;i++ )); do
	echo ${arr[${i}]}
done				(will output the number of total values in the array, then list each of the values one by one)

----

SHELL and ENVIRONMENT VARIABLES: (you can also think of them as local and global variables)

echo $BASH
export EPIC=Hello	(only maintains its value for the current shell session. If you logout and back in, it will lose its value. In other words, it is a local variable.)(To make it persistent, put that export line in your .bashrc file.)
echo $EPIC

----

SCHEDULED AUTOMATION:

sudo crontab -l	(to list the jobs for root. Do it without sudo to list it for the current user)
sudo crontab -e	(to edit the root's crontab. Do it without sudo for the current user's crontab)

cron: min(0-59) hour(0-23) dayofmonth(1-31) month (1-12) dayofweek(0-7) command

----

ALIASES:

alias desktop="cd ~/Desktop"	(you can do it straight from the command line, or put it in your .bashrc file)
source ~/.bashrc	(if you chose to put it in your .bashrc file)

----

WILDCARDS: (these are used directly in the commandline/bash shell/bash scripts, they are NOT regex)

? (single character), * (zero or more characters), [] (range, m[a,o,u]m is mam, mom, mum), {} (terms seperated by commas, i.e. cp {*.doc,*.pdf} ~),	[!] (NOT, so rm myfile[!9] will delete myfile1,myfile2,myfile3.. but NOT myfile9), \ (escape character)

Hello*.mp3

----

MULTIPLE COMMANDS: seperated with a ;
cd fuck ; ls ; cd ~ ; 	(does not actually need spaces at the end of each segment i.e. cd fuck;)
&&	(only run next section if the first section is a success)

-----------------------------------------------------^----------Bash Scripting Done.

MASTERING REGULAR EXPRESSIONS:

vim uses regular expressions for its back end, so there are a lot of similarities.

^ - matches/move to the start of a line, $ - matches/move to the end of a line

\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}	(to match and ipv4 address. \d{1,3} matches one to three numerical digits from 0-9. \. is an escaped dot., making it a regular literal period and not the metacharacter symbolizing any one character. \d matches a digit 0-9. {1,3} repeats the prior pattern one to three times. The {1,3} will not do anything by itself, because it only modifies what is before, and needs to look at what is before it in order to modify it.)

Regular expressions are used in programming languages such as Perl, Java, PHP, .NET and many more.
They are also used in text processing tools such as sed and awk.

There are BRE (basic regular expressions) which can only match one character at a time, and
ERE (extended regular expressions) which has features such as alternating, repetition and matching multiple characters. It also reverses the meaning of the \ escape with parenthesis() and curly brackets {}, so that instead of having to escape them out to remove their meta qualities like in BRE, instead you escape them to enable the meta qualities. This is designed to make using () and {} more intuitive. Also, in BRE we matched a digit with \d, whereas in ERE you have to use [:digit:] instead.
ERE builds off BRE, so it's just additional features on top of BRE.

-----------------------------------------------------^----------Mastering Regular Expressions Done. (video series was too based on Perl)

echo "This is a sed example" | sed 's/sed/good sed/'	(outputs 'This is a good sed example')

printf line1\\nline2\\n	(outputs 'line1' and 'line2' on seperate lines)
printf line1\\nline2\\n | sed '1d'	(deletes the first line, thus only outputting the 2nd line)

lsof | grep libselinux

----

sudoers file:

sudo visudo

john	ALL=(ALL)	ALL
%group	ALL=(ALL)	ALL		(if it's a group, it need a '%' in front of it)	can also john ALL=(ALL)	NOPASSWD:ALL if you want.

user/groupname,	What computer (hostname) you're allowed to access from (so ALL means that you can ssh in from anywhere and have full access), What user you are allowed to impersonate(this part must be in parenthesis() like seperating users with commas), what commands you're allowed to run (again, if it's more han one command then you put parenthesis seperated by commas), 

So in short: user/%groupname	hostname=(impersonated user)	allowed commands

Entries in the bottom of the sudoers file will override the entries above it.

----

sudo -s		(allows you to be root until you exit the session [-s is for session])

----

Most distros support creating a bin/ folder in your home directory, and it should automatically pick it up and put it into your PATH the next time you boot. You can do this so taht you don't disrupt other users of you want to install a newer version of a package normally used in /bin or /sbin, such as gcc. 

---

any xxxx.conf.d/ directories are used to override defaults	(such as xorg.conf.d/)

---

/var/log/syslog	(ubuntu)	(in redhat/centos, it's /var/log/messages)

cat /var/log/syslog | grep <applicationname>	(to see error messages pertaining to a specific application)

tail -f /var/log/syslog	(to 'follow' the log file live, showing new errors as they come up)

---

journald is running alongside syslog, so 2 different log files are being maintained. Some day in the future, syslog may go away.

journalctl	(lets you look at the logs inside of journald)

journalctl -k -b 1	(-k only show me kernel messages, -b show me from boot, 1 show me from previous boot [0 would be this current boot, 5 would be from 5 boots ago]) (you can just do 'journalctl -b' for this boot)

journalctl --list-boots	(shows how many boots the system has logged)

Currently, we should look in both syslog and journald/ctl, because while journald is SUPPOSED to centralize everything, it requires every application to adhere by that rule as send its data to journald, which is another matter.

journalctl -af	(-a for ALL the data, -f for Follow it live)

----

dmesg	(tracks the dbus [device bus, which manages all of the devices plugged into your motherboard]. Shows the device messages from boot.)

dmesg | tail (you can also do this right after you plugged in a usb device to find out its name [/dev/sdb for example])

ACPI is the power device used to toggle(?) the motherboard, communicating(?) with the motherboard.

udev identifies a device and assigns a name to it, as long as it is working and recognizable.

The entries (files) in the /dev directory could be pointing to real hardware, or a virtual (software or cpu-integrated) device. As an example, your random number generator could be a physical hardware card, or just software. To find out, type 'sudo rngd -v' to get info on it. If it outputs DRNG (deterministic random number generator), it means it is just software and not a hardware card.
So, oftentimes to troubleshoot and identify hardware, you need to use the specific tool for that device.

You can override udev to make the system behave in a different way. Specifically, if the name of the file/directory pointing to the same piece of hardware (such as a soundcard) has a different name on different distros, then udev was most likely overridden. udev config is in /etc/udev/udev.conf . In /etc/udev/rules.d/ you can find rule overrides by certain vendors, otherwise it is usually empty. 

/sys/ is a folder which contains mostly 'fake date', i.e. monitoring, statistics and performance information represented as files.

In the /sys/ folder, you can see any connected device, whether it has a driver or not. In other words, you can see what the system sees. However, as you go down the directory structures, the filenames can become a little meaningless to you, so you can use commands such as 'lspci' to abstract that and give you more information about each detected device. With that information, you can for example look for a suitable driver from the manufacturer's website.

lspci/lsusb/lscpu -v (or -vv or -vvv for more and more verbose information. A lot of commands do this.)

A lot of usb3 disks use the SCSI protocol (even though they are SATA disks) in order to work faster, so they show as SCSI disks on dmesg.

If you plug in a usb drive and it doesn't automount into your file manager, you can go to the disks utility and maybe find it there. 

lsmod	(lists the varies drivers [modules] that are currently attached to your kernel.

modinfo -d i915	(to see information about your video driver [will output 'Intel Graphics']. This one doesn't have much info, but others probably will.)

modprobe	(by itself, it just scans [probes] the hardware and loads the appropriate modules)
modprobe -v i917	(tells the system that you want it to load the i917 module. You can use this to force the kernel to load another module if it for some reason loaded the wrong one for that hardware, which shouldn't really happen.)
rmmod i915	(to remove the old, incorrect module that you are no longer using)

If you have hardware that doesn't match up to yout modules, you will not see that module in modprobe. You can find it in lspci (for example), and that's an indicator that you are missing the correct module, so you should go to the vendor and download the correct driver (which for linux should be in module form. After this step, 9 times out of 10 it should loud automatically. If not, you run the modprobe -v <modulename> command.


dmesg | grep - error	(to chech for errors, you can do this right after an install, just for posterity)

sudo apt upgrade (after you already ran sudo apt update) will upgrade all of your programs and support files, but NOT the kernel. To upgrade the kernel, you must run 'sudo apt dist-upgrade'. When you dist-upgrade, you need to reboot when it's done.

sudo apt autoremove	(to remove any packages that you don't need)

----

MANAGING SOFTWARE WITH APT:

apt is a front-end, user-friendly interface to apt-get and apt-cache

/etc/apt/sources.list and /etc/apt/sources.list.d/ 

---

Adding a security key for a third-party vendor's repository:

cd /root
sudo wget http://www.vendor.com/vendorsname-key.asc
sudo apt-key add vendorname-key.asc

----

CREATING USER ACCOUNTS:

sudo useradd joe	(does not prompt you for additional info)
sudo useradd -d /mnt/storage/home/joe -e 08/04/2017 -aG wheel -p "password123" -s /bin/zsh -u 510 joe	(-d override home dir, -e account expiration date, -aG append to secondary group, -g make primary group, -u set a user ID, -p set a password, -s set a default shell)
sudo userdel joe
sudo usermod -s /bin/bash joe	(uses all the same options as useradd to change any aspect of an existing user account, -L to lock account, -U to unlock account)
sudo chage -l joe	(to pull up expiration information about the user's password and account)(-M maximum age of password in days, -d set the date that the password was last changed [to affect the expiration countdown of the password])
sudo chage -M 90 -d 2017-01-01 joe

sudo vim /etc/login.defs	(default mailspool location, password expiration policies, user/group id numbering policies, UMASK, CRAETE_HOME yes/no, userdel autoremove emptygroups yes/no, password encryption method)
/etc/default/useradd	(defaults that override the settings in login.defs: GROUPid,HOME location, INACTIVE, EXPIRE, SHELL, SKEL, CREATE_MAIL_SPOOL)
sudo getent passwd joe	(can use this instead of 'cat /etc/passwd | grep joe')
You can use 'getent' with other files too, such as 'shadow'.

sudo groupadd sales	(create a group named 'sales')(Usually you do not need to use the options in the groupadd command, because nobody logs in AS a group itself. You can use the option -g to manually set a GID, which is only needed if you're trying to synchronize group numbers across different servers.)
sudo gpasswd -a joe sales	(-a to add a regular user, -A to add an administrative user, to add the user joe to the sales group [as a secondary group])
An administrative user	(sudo gpasswd -A bob sales) means that they are an administrator FOR THAT GROUP, so they can add or remove people from that specific group.
You can add one person to multiple groups, depending on what you want them to be able to access.
sudo gpasswd -d joe sales	(to remove joe from the sales group)

sudo chown -R joe:joe corpdocs/	(userowner:groupowner, -R to make it recursive so that it applies to all subdirectories and contents)
sudo chmod -R o-rx ./sales
sudo chmod -R g+w ./sales
sudo chown -R :sales ./sales	(to assign ownership of the sales directory to the sales group [previous permission configuration (rwx) will still apply, except it will now apply to the new group instead of the old one.])
You CAN nest the groups, for example putting the sales group inside the marketing group. This is normally done in fileservers.
You can also use the 'chgrp' command to JUST change the group.
newgrp sales	(for a non-administrative user to temporarily change his default group to sales, so that the new files he creates will be owned by the sales group)
groups joe	(tells you which groups joe is in. You can run this as a regular, non-admin user to see which groups any other user is in. 'groups' by itself shows you what groups you are in.)
All of the above are groups managed on an individual system. There is a way to make centralized groups to manage them across systems, which will be covered later.

---

ADVANCED (file) PERMISSIONS:

If you want the Sales group to have read access to the Marketing directory, and give no access to 'others' (everyone else), then you can use FACL.
FACL	(File Access Control Lists)(Allows you to give specific access to multiple different groups/users on any given file, and not be limited by user/group/other anymore.)
FACLs are turned on by a per mountpoint basis.
In the /etc/fstab file (in Redhat, Centos, Ubuntu, Debian) 'defaults' includes the enabling of FACL. If not, you can add ',acl' to enable it.
getfacl <filename>	(lets you view the acls. If you get an response, then you know that facls are enabled for that filesystem, even if you are not currently using them.)
If you are using FACL, then when you do 'ls -l', you will see an extra character at the end of the permissions. '.' means there is nothing else there, '+' means that there are extra permissions that FACL is using.
setfacl -m u:joe:r-x ./marketing (-s is to set a new access list [will overwrite if one already exists], -m is to modify an existing access list, -x is to remove an existing access list)(you can use -m even if you are setting one up for the first time, it is a safer option)(u is 'user', you can also type out the whole word 'user', 'g' would be for a group)(you can use commas [,] to seperate multiple entries, but it can get visually messy)
setfacl -m g:sales:r-x ./marketing	(to give the sales group r-x access to the marketing directory)
There is a 'mask' in FACL which sets the maximum permissions you are allowed to give. If it's rwx then it means you can give as much permissions as you want. If the mask is r-x, then even if you define rwx permissions, the write permission will not work. The mask is keyed from the (owning) user's permissions. If you later do 'chmod u-w ./sales', then the mask is automatically updated. The marketing group can still be described with 'rwx' permissions, but on getfacl it will show as a comment '#r-x effective', meaning they have instantly lost their write permissions.
If you gave youself/the owner permissions which affect the mask, FACL will protect from that by remembering the last mask and not increasing the access list's permissions, but only until the next reboot, so you need to keep track of it.
There are some commandline options for setfacl where you can tell it to recalculate the mask.
You can browse a directory if the directory has execute permissions. You may not want to set execute permissions on files because someone could run a script from that directory then.
When you setfacl, then subdirectories and files created respect the UMASK, HOWEVER you can also change the defaults of FACLs which will change that dynamic by applying a new default to use instead of the system UMASK. 
setfacl -m d:g:marketing:r-x ./sales	('d:' for setting as default permissions, g for group [leave next field empty for all new groups])(so now any folder created in the sales/ directory will automatically have the group 'marketing' attached to it with r-x permissions)
setfacl -m d:g::r-x ./sales
setfacl -m d:u:joe:r-x ./sales
So with the above, any subfolders and files in the sales/ directory will inherit these access lists.
'getfacl ./sales' will also show you all of the defaults, each on a line beginning with 'default:'

STICKY BIT:
If you have 'write' permission on a directory, you can delete files inside that folder.
The sticky bit prohibits somebody from deleting or renaming files they don't own.

sudo chmod o+t ./sales	(to set the sticky bit on the sales directory)

----

PARTITIONING:

sudo fdisk -l	(lists all of the disks in your system [also shows the partitions of each disk and some more info])
Parition numbers start with 1 (sda1) because sda0 is just a tie-in to the MBR (Master Boot Record), which is a very small parition that just lists out the other partitions.

fdisk /dev/sda	(to start editing the partitions on the disk)(fdisk uses MBR)(The only commands you need to remember are 'p' to show the parition table, 'n' to create a new partition, 't' to change the filesystem type of that partition, and 'w' to write the finished parition table to disk. For size, do for example +100G.)

gdisk /dev/sda	(same as fdisk, but for GPT instead of MBR. gdisk was designed to work the same way as fdisk so you don't need to relearn commands.)

'parted' is a little more powerful than fdisk and gdisk. It can do both MBR and GPT. It also allows you to resize partitions, which you can't do in fdisk or gdisk. In the parted prompt, type 'mkpart' and follow the prompts.

FORMATTING (adding a filesystem):

mkfs.ext4 /dev/sda1	(OR)	mkfs -t ext4 /dev/sda1 (-t for type)
mkfs.xfs /dev/sda1
mkfs.btrfs /dev/sda1
resize2fs	(is a utility that you can use to resize ext2,3 or 4 partitions)

swapon /dev/sda3	(to activate a swap partition after you've formatted it first)

mount /dev/sdb1 /mnt/sales
mount -t xfs /dev/sdb1 /mnt/sales	(if the above didn't work, you may need to specify the filesystem type manually like this. But it usually automatically figures it out.)

chmod -R +rw /mnt/sales	(to give EVERYBODY read/write access on that directory)

Then, you can add the following in /etc/fstab :
/dev/sdb1	/mnt/sales	ext2	defaults	1 2
(if you just want it up and running, use 'defaults') (first '1' is to enable dump, which is defunct now) ('2' is low priority for filesystem check, '1' high priority [usually set on root], '0' is never check [not recommended])

You want to always (typically?) mount things (disks and partitions) to an empty folder, because if the folder already has data in it then that data will be inaccessible while the new disk is mounted there.

----

USING LVM:

When you do 'mount' to look at all mounted filesystems, the lines that begin with '/dev/mapper/' are using LVM. Also, instead of /dev/sda1, you'll see /dev/mapper/fedora-root or /dev/mapper/fedora-home, so that makes things more immediately obvious.

When you install LVM, make sure you look for the package 'lvm2', which has many more features than the older version of lvm.

All the commands start with the letters lv, pv or vg (logical volume, physical volume, volume group).

First step:

pvcreate /dev/sdb1 /dev/sdc1	(Adding physical volumes to lvm. Use partitions, not just the disk itself. You can add multiple volumes at the same time. It is now going to erase [wipe] all listed partitions, because it only needs the partition, not the filesystem. LVM formats the partitions its own way when it creates the physical volumes.)

pvdisplay	(to double-check whether the partitions were indeed added to lvm as physical volumes) ('pvs' command is the same, but less info)

vgcreate vg1 /dev/sdb1 /dev/sdc1	('vg1' is just the default naming convention, can be substituted for any other name you choose. We have now created a volume group comprised of those two physical volumes.)

vgdisplay	(double-checking to see if the volume group vg1 did actually get created. It will now show you the volume group size as the combined size of the two physical volumes. You can always add another physical volume to the volume group later, as needed.)

lvcreate -L 250G vg1 -n website		(create [slice] a new 250GB logical volume [-L is size] from the 'vg1' volume group, and name that logical volume 'website' so that you easily know what it is.)

lvdisplay	(double-checking it. It will read '/dev/vg1/website'. You can use this convention instead '/dev/mapper/' so that it is easier to remember. It will also automatically generate its own UUID.)

mkfs.ext4 /dev/vg1/website	(to format it as ext4 or whatever filesystem you choose)

cd /mnt; mkdir website; mount /dev/vg1/website /mnt/website	(to mount it wherever you want)

All done! Now we can 'cd' into that folder and start working.

Now you can start to manage permissions if necessary.

To grow the size of your current lvm in the future:

pvcreate /dev/sdd1	(first, to give a partition to lvm as a physical volume)

pvs	(to double-check that it was brought in. This is another kind of view from the 'pvdisplay' command.)

vgs	(can be used instead of 'vgdisplay' to show a more concise view)

vgextend vg1 /dev/sdd1		(use the 'vgextend' command to add a physical volume to an existing volume group)

vgs	(double-check that the capacity of the volume group was extended)

lvs	(to check the status of the logical volumes, can be used instead of 'lvdisplay' for a more concise look)

lvresize -L +250GB /dev/vg1/website	(note that you need to use the 'lvresize' command, and also the '+' to add size to the '/dev/vg1/website' logical volume, or it will overwrite your existing data)

lvs	(double-check that your logical volume was in fact resized like you wanted)

resize2fs /dev/vg1/website	(because the ext4 filesystem won't recognize that the logical volume has grown until you do this step)

With LVM, you can resize your volumes WITHOUT taking them offline or unmounting, which is a big deal for online systems/servers. Tools like resize2fs (used on non-lvm volumes) need to unmount the volumes before they can be operated on. But if you use resize2fs on an lvm volume, it does NOT need to unmount.

Finally, to make it permanent, go to /etc/fstab and add:

/dev/mapper/vg1-website		/mnt/website	ext4	defaults	1 2		(The 'vg1-website' comes from 'ls /dev/mapper/'. It's the true name of the logical volume that you created.)

Now, if you want to REMOVE and TEAR DOWN the whole LVM setup, first you unmount, then you remove them in the opposite order that you created them:

umount /mnt/website

lvremove /dev/vg1/website

vgremove /dev/vg1

pvremove /dev/sdb1 /dev/sdc1 /dev/sdd1

pvs; vgs; lvs	(to double-check that nothing remains)

Done. As a footnote, btrfs already has lvm-like features built-in, so if you use it you will not need lvm.

------------------------------------------------------------^---------ITPRO Sysadmin Done.

ITPRO Linux Power User:

The 'type' command works on both commands and aliases.

set $PATH=$PATH:/home/joe/bin		(this is temporary, it is lost when you logout)

!37	(run history command number 37)

!type	(looks for the most recent entered command that starts with the word 'type', and runs it)

!type mo	(spaces work here too, for example if u are trying to remember/rerun 'type more')

set | grep PATH		(to search through your variables)

!?exit?		(looks for a command that has 'exit' somewhere in the middle, or even at the end)

!!h		(run the last command i just ran, but at the letter 'h' to the end of it [i.e. 'ls -l' becomes 'ls -lh'])

fc 49		(opens the 49th command in history in nano, allows you to edit it as a script, and then runs it on exit, if you save it)

fc 63 66	(does the above with commands 63 to 66 in seperate lines)

dmesg	(shows you all the hardware detection that happens when you system boots up)

passwd is sorted by order of account creation.

chsh -s $(which zsh)			(the parenthesis in linux is like the mathematical order of operations, it runs that first)($ is placeholder variable)(this is how you 'nest' commands, i.e. run one command inside of another)

echo "I am $[2017 - 2007] years old"	(will output 'I am 10 years old')(we use square brackets[] here because it is already enveloped in quotes)

echo "There are $(ls | wc -w) files in this directory"	(we go back to parenthesis() here because it is a command)

set		(lists all the variables that were created in the system)

With the bash shell, logging in is what triggers it, otherwise changes will not show up when you check them.

alias mgtowls='ls -lah'

In most distros, aliases can be found at the end of the ~/.bashrc file.
In the .bashrc file, there are two PS1 variables (in an if/else loop): one for a color prompt and one for monochrome.
In the PS1 variable, the '\' expands different options/variables. For example, '\#' shows how many commands you have run since you logged in. '\$' shows the user prompt ($ or # prompt). '\$ ' for a space after the prompt. '\w' shows the full path. '\W' shows only the current subdirectory. '\\' to display a backslash. '\d' for the date, '\t' for time. '\h' for hostname. '\s' shellname. '\n' for newline (to make a multi-line prompt). '\u' is username, '\u@\h' to display as 'username@host'.

tar -xvzf <archivename>	(to unzip an archive)
tar -cf <newarchivename>.tar <file/directory1> <file/directory2> 	(create a tar archive from the mentioned files/directories)

man -k <command/file>	(to find entries in alternate man pages [1-8], or other commands with matching keywords in their description)

---

FILESYSTEM HIERARCHY:

/initrd.img	(the initial bootloading image. When Ubuntu starts up, initrd.img is loaded into RAM, and from there it starts loading the services and systemd) 
/vmlinuz (the kernel) is also found in the root directory in Ubuntu. Other distros may put these files in the /boot folder so they are neatly out of the way. Note that these two files are actually just links pointing to the actual files in the /boot folder.
/bin contains binaries that are critical to the functioning of the operating system, such as 'echo' and most of the filesystem commands (cp, mount, more).
/sbin has support binaries which are not critical, such as 'wpa_supplicant, fsck.ext4, tune2fs, mkswap'. It's important, but not so critical that you can't boot up without it.
/usr contains "user stuff", including user binaries. In some distros, /usr is readonly.
If we were following the FHS TO THE LETTER, then /bin and /sbin would be OS commands designed for the administrator. /bin would be critical, /sbin would be non-critial. /usr/bin and /usr/sbin would be commands for regular users (not administrators) and it would be readonly and not changeable. Thus, all user-installed software would be in /opt. However, many distros mount /usr as writable, so we can just use that for user software if we're not being so strict.
For many distros that have mounted /usr as readonly, they will mount /usr/local as writable. That way, you are able to install applications there.
Originally, /usr/local was intended for programs to be run on the local machine, and /usr/share was for programs that would be shared and run by multiple machines over the network. However, since these days pretty much everything can be run over the network, we don't normally differntiate between the two anymore, but you will see the local and share folders from time to time.
/boot is super important. If you delete /boot, your computer won't boot. Boot files are stored inside there.
When you install linux, there is always a bootloader installed. It is either installed in the MBR for the old style, or in the EFI folder if your system uses EFI.
/boot/grub is the main grub folder. /boot/efi will be empty if you are using MBR.
/media is used for removable media, because /mnt is usually meant to be used by administrators, so it can give regular users some permission troubles.
/var is where applications store their (usually temporary) data, ranging from log files to apache websites. Files can actually stay in /var for as long as you want them to. Most services also store their actual configurations inside of /var.
 /tmp is for truly temporary files.   
/proc is short for 'processes'.
/run in ubuntu is usually temporary stuff for users. You can find a mount, or data stored by other programs there.
/srv used to be for services, though it's usually nonexistent/empty or just there for backwards compatability. Most services store their information in /var nowadays.

Slackware is a distro that very strictly adheres to the FHS. Ubuntu is quite loose with it, and Redhat pushes for many changes to it.

---

mkdir "Junk Stuff"	(quotes "" are one way of dealing with spaces)

ls dir?		(will match 'dir1' 'dira' [exactly ONE character wildcard])
ls dir[a-z,A-Z,0-9]	(will match ONE alphanumeric character [dir1, dira, dirZ])
In the brackets[], comma(,) for single entries, and dash(-) for ranges (this is all just in the basic shell).

---

ls > output.txt		(the file can be created on the spot, does not need to be preexisting)
> is the same as 1> (first output), which is standard output. 2> is second output (errors).
&> is both output AND errors.

ls &> output.txt
ls | tee output.txt	(same as above &>, but on top of that ALSO outputs everything to the screen)
sort -r < output.txt

touch memo{3,4,5}	(creates files 'memo3 memo4 memo5')
touch {tom,dick,harry}-memo{1,2,3,4,5}	(creates 15 files, from tom-memo1 to harry-memo5)
touch memo{1..9}	(creates 9 files, from memo1 to memo9)

chmod g+w,o+rw <file/dirname>

/etc/login.defs		(for the UMASK and other definitions in Debian/Ubuntu. For Redhat, it's in /etc/profile)
Also, users can override the default UMASK by setting their own UMASK in bashrc.

chown joe <filename>	(requires you to either have permissions to the file [whether user,group or other] or superuser privileges)
chgrp sales <filename> 	(file ownership always takes precedent over permissions ["you're the owner, do whatever you want"])
sudo chown joe:sales <filename>

---

mv note{1..499} memo{1..499}	(to batch rename files)

cp -R (recursive) is useful, but with 'mv' you do not need to specify since in that case it is already assumed to be recursive.
cp -u ./memo1 ~/backups	(-u is for 'update', i.e. copies it ONLY if the source file is NEWER than the destination file)

'rm -r <directoryname>' usually works, but on occasion might give you an error if there is a file in that directory that you do not have permissions to. In that case, assuming you have permissions on the folder but not the file inside it, you can use 'rm -rf <dirname>' to force delete it. This overrides files with readonly on them. A regular user can only really do this in their home folder, but a root user can do it anywhere in the filesystem.

---

In vim:

:856,888d	(delete lines 856 to 888)(can use y to copy instead of delete, or any other command)
:set number (turn on linenumbers)(temporary)
:set number!	(turn off linenumbers)

---

FINDING FILES:

'which' only shows you the first instance of a program.
'whereis' shows you every instance, including files other than the executable binary (man page, libraries, etc..)

Even the root user is prohibited from being in certain directories.

find ~/ -name fuckinfuckwads.txt 2> /dev/null

grep Crane /Documents/*	(you need to specify a file, or files with '*', because grep will not work on directories)

grep -r (to make it recursive, i.e. look into subfolders)
	-v to exclude certain characters/strings

---

MANAGING PROCESSES:

ps -A	(show all processes)
ps -ef	(show all processes but with more information [including which command is ran, who the user was, PID, process runtime])
ps -u <username>	(show processes that a specific user is running)
ps -xf	(show process heirarchy)

pgrep <programname>	(ONLY shows the PID)
pgrep -u joe <programname>

bg 1 	(can be used to run a suspended job in the background)
fg 1	(to bring that job into the foreground)
sudo <command> &	(to run the job in the background without having to go through the steps of suspending it and typing 'bg 1')
%4	(bring job 4 to the foreground)
nice -n sudo md5sum /dev/sda1	(to make the command run at a lower priority [in the foreground])(add a '&' to make it run in the background)
'nice' to launch a new command, 'renice' to change the priority of an already running program.
sudo renice -n 10 4928 (4928 is the PID [you can use other identifiers], 'sudo' if the command is running as root)
sudo kill 4571
kill -l (to show you a list of signals you can use)

----

EXECUTING SCRIPTS:

bash ./myscript.sh	(to execute a script that does NOT have executable permissions set on it)(bash is the interpreter)
./myscript <argument1> <argument2>
./myscript "<multiword argument>"
echo "$1 $2"
echo "The date is `date`"
read -p "Enter your first name: " FNAME	(-p is to prompt you for data instead of just expecting you to start typing without instruction)(FNAME is a variablename you created to store the user input at that point.)
echo "Your first name is: $FNAME"
In this case, FNAME was created on the 'read' line, but you can instead create it before by typing 'FNAME=$1' (without the single quotes) to automatically take the user's first argument/input and put it in a variable named FNAME.
DATE=`date`	(note the backtick for command)

---

#!/bin/bash
read -p "Enter first number: " NUM1
read -p "Enter second number: " NUM2
let RESULT1=$NUM1/$NUM2		('let' tells the system that the variable is not finished and still needs to be interpreted)
echo "Bash answer: $NUM1/$NUM2=$RESULT1"

RESULT2=`expr $NUM1 / $NUM2`	(expr is a command that lets you run mathematical expressions. This is just a different way to do RESULT.)
echo "Expr answer: $NUM1/$NUM2=$RESULT2"

RESULT3=`echo "$NUM1 / $NUM2" | bc`		('bc' is a commandline calculator that accepts arguments)
echo "Bc answer: NUM1/$NUM2=$RESULT3"

These are just 3 different ways to achieve the same thing in a bash script.

----

PROGRAMMING CONSTRUCTS (if, while, case):

---

#!/bin/bash

FNAME=$1
LNAME=$2
if [ "$FNAME" = "" ]; then			( '[]' because it is a test. If the FNAME variable is empty, then prompt for a first name.)
	read -p "Enter first name: " $FNAME		(In a script, anything within square brackets [] is considered a test.)
	read -p "Enter last name: " $LNAME
else
	echo "Your firstname has been obtained from the commandline"
fi
echo "Your name is: $FNAME $LNAME"

Now, when you run the script, you can give it arguments and it will use them. If you didn't give any arguments, it will prompt you for them.

---

IF has three parts: if, then and else.

Within square brackets [], -a and -e test if a file exists, -d tests if a directory exists, '!=' (without the quotes) means NOT EQUAL to.
DIRNAME="/home/(`whoami`)/Backups"	((`whoami`) grabs the username and inserts it there)(we use this method because '~' expansion didn't work
					 in the shell script)
[ -e $DIRNAME ] && echo "Directory already exists." || mkdir $DIRNAME	(tests whether the directory named by the variable DIRNAME exists)
							( The '&&' can be used instead of 'then' in this case, and '||' instead of 'else'.)
Thus, you have made an if/then/else loop in just one line using the above notation ( [] && || ).

---

#!/bin/bash

case `date +%a` in		(when you call a command in a loop like this, make sure to use backticks)
	"Sat")		(This is written in the same format that the 'date +%a' command outputs i.e. Mon, Tue...)
		tar -cvf /home/joe/Backups/weekly.tgz /home/joe/Documents
	  ;;		( ';;' means that is the end of this case [case closed])
	*)		( '*)' means for every other case that is not explicitly mentioned i.e. other than "Sat")
		tar -cvf /home/joe/Backups/daily.tgz /home/joe/Documents
	  ;;
esac

---

for NAME in Ralph Don Mikey Leo ; do
	echo "$NAME is my favorite turtle."
done		(will run the 'echo' command 4 times in a row, one for each turtle)

---

for FILE in /home/joe/Documents/*
do
	tar -czf $FILE.tgz $FILE	(To keep this example simple, this puts the compressed files in the same directory as the original files
done					 are. If we did '/home/joe/Backups/$FILE.tgz' it would error out by repeating the full path twice.)
		(This script will also not work for files that have spaces in their name, we would have to do more for tar to handle it.)

*TRY TO USE DIFFERENT FOR LOOPS ON FILES WITH A SPECIFIC NAMING CONVENTION IN A LARGE FOLDER (SELF STUDY)*

---

A while loop will run while a test is true, and it stops when it becomes false.
An until loop will run while a test is false, and it stops when it becomes true.

---

N=0
while [ $N -lt 10 ] ; do		(you can use a < instead of '-lt' here)
	echo -n $N
	let N=$N+1
done			(will run 10 times [from 0-9] and then stop)

---

N=0
until [ $N -eq 10 ] ; do
	echo -n $N
	let N=$N+1
done			(Will do exactly the same as the above script. The only change was -lt to -eq in until.)

------------------------------------------------^------- ITPRO Linux Power User Done.---

journalctl --since "10 minutes ago"	(OR check /var/log/ [syslog or other files] )
sudo systemctl status apache2
sudo systemctl disable apache2

sudo chmod +s /dir/name/	(becomes drwsrwsr-x)

nmap -v -sT localhost	(scans your own computer for open ports)(-v is verbose, -sT is tp scan using TCP/IP)
systemctl disable apache2	(closes port 80)

nmap -v -sT <website address>	(shows you what is visible to on that server from the network)

---

When using Docker, 'apt-get' is more reliable than the more heavily scripted 'apt'.

---

PLURALSIGHT LINUX TROUBLESHOOTING AND MAINTANENCE:

/proc/ describes the running processes and some system resources.

ls 4633		(or whatever PID. Here you'll find datafiles pertaining to the process. Some files are common to any process, like cgroup and
		 status, others will be unique to the particular process.)
journalctl _PID=74344	(will show you any log entries associated with the process)

ls -p /proc | grep -v / | column	(-p adds the '/' symbol to directories)(this makes it easier to see files without directories)

cat /proc/uptime	(shows you the number of seconds from this boot, and the sum number of seconds your cores have been idle)

less /proc/filesystems	(shows you all the possible filesystems your kernel supports)
less /proc/meminfo
less /proc/cpuinfo

df displays each disk parition recognized by your system, along with its total, used and available sizes, and its mountpoint.

snap creates a virtual parition for each package. That's great from a security and process effieciency standpoint, but awful for viewing paritions quickly though df.

df | grep -v snap

df -i (to show inodes instead of space)

A server can switch to readonly if you run out of inodes.

systemctl disable/enable (for start at bootup) start/stop (for immediate start and stop)

sudo dmidecode -q | less	(-q for quiet. dmidecode puts your hardware information in a human-readable format.)
	(-s to filer output by specific strings, i.e. -s chassis-type, baseboard-product-name, processor-frequency)
	(-t for type of entry i.e. -t bios, slot [for available pci slots])

sudo lshw -C disk	(-C is class)
sudo lshw -short	(quick summary organized by class)

---

At grub, press Shift during boot and select the recovery mode. From here, you cn choose to only enable services you need, update grub, and/or drop to a root shell.

---

sudo smartctl -i /dev/sda	(-A for all avaiable attributes on the device. Does not show test results.)
sudo smartctl -l selftest /dev/sda	(access previous test data from selftests)
sudo smartctl -l xerror /dev/sda
sudo smartctl -l devstat /dev/sda

sudo smartctl -t short /dev/sda		(to perform a short test)

----------------------------------------------------------------------------------

LINUX ESSENTIALS:

Variable names must start with a letter, have no spaces, and no punctuation marks.

printenv	(displays all the environmental variables)

echo "I have \$1200"	(outputs 'I have $1200')(need to escape the '$' so that it is not treated like a variable name)

Wildcards: b{ao}{ow}l	(will match 'bowl', 'bawl' and 'bool')

Case sensitivity is a function of the linux filesystem, not of the operating system itself. Though many linux systems will assume case sensitivity even if you are accessing a windows filesystem such as FAT32 or NTFS.

ls | head -5 | tail -2	(outputs the 4th and 5th entry [the last 2 of the first 5])

Regexp: * is 0 or more, .* is one or more. If searching for a literal dot, like in a file extension, first escape the dot with a \.

grep -n 'melon' fruitstand.txt	(-n shows you the line numbers and matches)

grep -E '[aeiou]{2,}' fruitstand.txt	(outputs strings of two or more vowels in a row)
grep -E '2.+' fruitstand	(outputs any lines with a '2' in them, but do not end with a '2'. '.+' means the preceding character matches one
				 or more times.)
grep -E '2$' fruitstand	(a '2' at the end of the line)

grep -E 'is|go|or' fruitstand	('|' is for 'or', so matching either of those 3 options)
grep -E '^[A-L]' fruitstand	(matches lines that begin with the letters A through L)
grep -E '^[^A-L]' fruitstand	(the '^' inside of a bracket means 'not' or 'inverse of')

---

if [ -s file ] 	(checks to see it filesize is greater than zero)
if test -s /tmp/file	(same as above)

In conditionals, && means both sides have to be true, || means only one side has to be true.

---

for d in `ls *.wav`;
do aplay $d
done
 
---

seq 1 10	(count from 1 to 10, putting one on each line)
seq 1 2 10	(the middle value is increment amount, will count by 2s)

---

functionname()	{
commands
}

Functions are only run when they are called from the main body of the script itself.

---

termcause = 0		(maybe he forgot the single quotes around the 0, or not?)
exit $termcause		(the termcause variable can be overwritten with an errorcode depending on what happens in the script)

---

backup script: (where $1 is foldername and $2 is today's date)(can run by './backupscript.sh Documents 07-20-19)
mkdir $1_$2
cp -R $1 $1_$2
echo Backup of $1 is now complete.

---

ip address show	(shows you layer 3 routing)
ip link show	(shows you layer 2 linking)

sudo ip link set enp0s3 down	(to turn off the wired connection)(same but with 'up' instead of 'down' to reconnect)

sudo ip addr add 192.168.1.10/24 dev enp0s3	(to add a new IP address to a device)(so if you previously had one address, you will now have 2)
	(can use 'delete' instead of 'add' to remove an ip address from a device)

---

grep '^joe' -nu /etc/passwd 	(-u is for unix-style offsets, -n is for line numbers, ^joe is for lines beginning with joe)

---

sudo passwd joe	(as a root user, you will not be prompted for the old password. You can directly create a new password without knowing his old one)

sudo groupmod -n joey joe	(change the group's name from joe to joey)(sudo groupmod -n newname oldname)

sudo deluser --remove-home joe	(to completely remove a user from the system)
sudo userdel -r joe	(another way to do the same thing)
/var/log/auth.log is a good place to check what users have been doing in the system, if there's anything shady.

sudo groupadd students	(to add a group named 'students')

sudo usermod -aG admin joe	(add joe to the admin group as a secondary group)

grep admin /etc/group	(see what members a certain group has)

sudo groupmod -n Staff admin	(change the name of the 'admin' group to 'Staff')

The (user)owner of a file/directory can only add the group membership of groups he is already a member of to that file/directory.
If you want to change (user) ownership of a file, you have to do so as root, because you need to have both your and the new owner's owner permission, and a regular user cannot be two users at the same time.
Also, even if you give some user/groups permissions to a file (as root), they will not be able access it if it is inside a directory that they do not have access to in the first place.

---

A named pipe (shown as a 'p' before the permissions) allows two running programs to communicate with each other in a one-way fashion.
A socket (shown as an 's' before the permissions) is similar to a named pipe, but allows network and bi-directional links.

---

Directories use the 'execute bit' to grant permission to search the directory.

Directory write permission allows users to write to that directory, create/rename/delete files within that directory, even if they do not have permissions to the files themselves, just the directory.

Modifying the file itself requires write access to the file itself, but deleting the file/creating a file only requires write access to the directory it is in.

Permissions on sumbolic links are (almost) always 777. This ONLY applies to the link file itself though, NOT the file being linked to. So the linked files can still be protected using their own permissions.

Many of the permissions do not apply to root, because the root is super powerful.

---

With a sticky bit set on a directory, a user may only delete a file within if he owns the file itself, or owns the directory it is in. The code for the sticky bit is '1', so a permission would be '1755' for example. Setting permissions as '0755' will remove the sticky bit.
chmod o+t	(another way to set a sticky bit)(chmod o-t to remove the sticky bit)

---

SUID (set user id) bit runs the program with the permissions of the file owner, not the user running the program. This can be used to give root access when needed for certain files (such as a file which needs to have its password changed [with 'passwd'] by a non-root user).
For instance, if a file is owned by root, and has the SUID bit set, then the program will run with root permissions and therefore it can read any file on the computer. Some servers and other system programs will run things this way, and they are often called 'SUID root'. SUID programs are indicated by an 's' in the owner's execute permission bit (rwsr-xr--).

SGID (set group id) sets the group of the running program to the group of the file (in the same way as SUID, but group instead of userroot). In other words, it's a group-level execute permission. This is indicated by an 's' in the group execute bit (rwxrwsr-x). 

4 = SUID, 2 = SGID, 6 = both SUID and SGID, 1 = stickybit. ex 4755
u+s = SUID, g+s = SGID, ug + s = both SUID and SGID.			(these are all used with 'chmod')

---

You can actually type out 'chmod rw-r-xr-- <filename>' and it will work.
chmod 1644 <filename>	(to set stickybit)

---

When using wildcards with ls, you can use 'ls -d' to search only for matching subdirectories and not the contents of those subdirectories.

---------------------------------------------------------------^-----------Lynda Cert Done.-------

ldd /bin/bash	(see what dependencies a program has [as libraries], in this case dependencies for /bin/bash)

LFCS Sander Van Vught: 82

ADVANCED FILE PERMISSIONS:

				file							dir

SUID		file(command) will be run as the owner of that file(command)*		- (no meaning)

SGID		file will be run as the file's group owner*		new files will inherit the directory's group owner
										(useful in a shared group environment)
stickybit			- (no meaning)				only the (user)owner can delete files within the directory

	* means that it is dangerous and you will very rarely use it as a sysadmin

---

chmod +t *	(to add the stickybit to all items in that directory)

By default, the stickybit is applied to the /tmp directory, so only a user that has created a file in /tmp is allowed to delete that file.

---

ACCESS CONTROL LISTS:

There are 2 types of ACLs: normal (applied on existing files) and default (applied on files which are created in the future)

setfacl	-R -m g:sales:rx <filename>	(setfacl = set file access control list, -R = recursive, -m = modify)
setfacl -m d:g:sales:rx <filename>	(the 'd:' at the beginning is to make it default)(you can add -R before -m for recursive)

Use BOTH of the above lines, one after the other, first the normal, THEN the default.

ACL depends on the filesystem itself. Modern linuxsystems already have this, but if not you will need to set the 'filesystem: acl' option. This allocates extra space on the filesystem for ACL support. If this option is not activated, you will get an 'operation not supported' error.

getfacl <file or dirname>	(to see the access control list for that file/directory)('mask' is handled automatically by linux)

Reminder: if you just have 'read' permissions on a directory, you cannot do anything. You need 'execute' to be able to get into that directory.

----

EXTENDED ATTRIBUTES:

Automatic on most modern linux distros. If not, you need the filesystem option 'user_xattr' (at fstab).
Extended attributes also apply to the root user, however the root user can change the attributes.

chattr, lsattr

chattr +i <filename>	(sets the 'immutable' attribute on the file, so that nobody can delete the file)
lsatttr <filename>	(see which attributes have been applied on the file)
charrt -i <filename>	(removes the 'immutable' attribute from the file)

----

QUOTAS:

Filesystem quotas apply to users, not to directories. If you want to control the amount that can be written to a directory, just use a different parition and size it appropriately.

However, in btrfs you can indeed set quotas for not just users but also directories themselves, using a completely different set of commandline utilities.

---

Quota management on EXT4:

First, install the quota package.
(Later, on each directory where access is restricted, you will need to generate two files: aquota.user and aquota.group.)

Go to /etc/fstab and set the options 'usrquota,grpquota'.

Then make sure the filesystem itself is accessible. ('chmod -R 777 /<mountpoint>' is an ugly way to do this)

mount -o remount /<mountpoint>	(to make sure that your new filesystem is activated with the right options)
mount	(to doublecheck that the 'usrquota,grpquota' options are activated on the appropriate partition)

quotacheck -mavug	(scans the /etc/fstab for quota-enabled partitions, and populates those blocks with the aquota.user and aquota.group files.)

quota -vu <username>	(to show current quota settings for that user)

-

quotaon	(needs to be started at each boot. You can either manually start it at the commandline each time, or start it as a default service) 

quotaon -a	(activates the quota system on all paritions that support it)

edquota -u <username>	(or -g <groupname>)(opens an editor where you can specify the quota settings)(you can specify by blocks or number of files. No matter what blocksize is used in a filesystem, quota uses 1k as the default blocksize.)
The soft limit can be exceeded temporarily, based on the 'grace time' setting which by default is one week.

sudo edquota -p <reference_user> <destination_user1> <destination_user2>	(to duplicate a user's quota policy and apply it to other users)

edquota -u joe

repquota -aug	(provides an overview of quotas used/remaining by each and every user)

(LFCS likes ext4, so this is the most likely version to be given in the exam.)

---

Quota Mangement on XFS:

mount -o uquota /dev/xvm/home /home	(will mount that device on '/home' with the 'uquota' option)
xfs_quota -x -c 'limit bsoft=500m bhard=550m tanya' /home	(to set the quotas)
xfs_quota -x -c report /home		(to check the status)

----

FINDING FILES WITH A SPECIFIC PERMISSION SET:

find /etc -perm 0600 -exec ls -l {} \;	(find ONLY files matching EXACTLY 0600 permissions)
find /etc -perm /0600 -exec ls -l {} \;	(find ANY files that have a '6' in the user permission)

find / -perm /4000 -exec ls -l {} \;	(to look for the SUID [set user id] bit)

----

NETWORKING:

dhcp = Dynamic Host Configuration Protocol (can be used by a sysadmin instead of manually setting up every node's connection: IP Address, Subnet Mask, Default Route, and DNS)

The 'dhclient' utility will contact the DHCP server (which is available somewhere on the local network) to configure each client's computer (node) automatically.

---

IPV6: Addresses are 128-bit and written in hexadecimal. Leading zeroes (as well as long strings of zeroes) can be ommitted. When referring to address + port, put the address between square brackets i.e. [fe80::1]:80 . Default subnet mask is /64. The node bit of the address may contain the device's MAC address.

fe80:: address is used by default. It is for internal communications only, it cannot be routed so it is not very useful.
As alternatives, you can use static addressing (manually entering addresses yourself), or use DHCPv6 (where multicast is sent out to ff02::1:2 port 547/UDP, which is the all DHCP multicast group, where the DHCP server will send a packet back to client 546/UDP), or use SLAAC (Stateless address auto configuration, where router solicitation is sent to ff02::2 which is the all routers multicast group, where the router sends back an IP Address which allows the host to learn the network prefix. This is done using the radvd package from the repository). The SLAAC method is very convenient because the ipv6 node learns the ip address automatically just by multicasting to the router, so as an admin you will not have to do any configuration at all.

----

APPLYING RUN TIME NETWORK CONFIGURATION:

Run time configuration VS persistent configuration. Run time is what you need to monitor your current settings and to test.

The 'ip' command is what is used to manage run time networking.

ip link show	(only shows information about network interfaces, not ip addresses)

ip address show	(lists all the interfaces and ip addresses that are currently used) 

ip address add dev ens33 10.0.0.10/24	(to set a secondary ip address on the interface)
ip address show	(to verify that the secondary ip address has been added)
ping 10.0.0.10	(to test that the secondary ip address is actually working)

If you set an ip address using this command, IT WILL BE LOST AFTER A REBOOT.*** That is why you need to make it persistent.

ip route show	(will show you the current routing configuration, as 'default via 192.168.4.2' or whatever address. This should be the identity of your router. Then, you can use 'ping 8.8.8.8' to see if you can indeed reach the outside.)

Run time configuration works pretty similarly between the different linux distributions, as opposed to persistent configuration which differs.

---

Note: 'biosdevname' is the new naming convention for network interfaces so that the name describes the physical hardware (eth0 was the old way). You can disable biosdevname in grub and fallback on the default old naming convention.

---

APPLYING PERSISTENT NETWORK CONFIGURATION in Ubuntu: 
 
/etc/network/interfaces	(is the file which contains the configuration)(it sources the contents of /etc/network/interfaces.d/, which is usually empty. You can put configuration files in this directory, but you don't have to.)

Loopback network interface in /etc/network/interfaces:
auto lo
iface lo inet loopback

Primary network interface in /etc/network/interfaces:
auto ens33
iface ens33 inet dhcp

To change the primary interface to static, you can do:
auto ens33
iface ens33 inet static
address 192.168.4.250
netmask 255.255.255.0
gateway 192.168.4.2

Then, on the commandline do 'ifdown ens33' then 'ifup ens33' to activate the interface. Verify with 'ip a'.

----

MANAGING HOSTNAMES:

Just write the hostname in the file /etc/hostname
You can also find the hostname in the file /proc/sys/kernel/hostname

In the file /etc/hosts, you can set local resolving of ip addresses. You can edit that file and add '192.168.4.240 unbuntumachinename' (without the quotes). You can check this by running 'ping ubuntumachinename' and it will work by pinging your assigned ip address.

It is not a bad idea to always modify your /etc/hosts file if you also manually modified your /etc/hostname file.

If at anytime there is a conflict between the DNS and /etc/hosts, /etc/hosts will win. You can use this to do basic censorship of urls on your computer.

---

MANAGING HOSTNAME RESOLUTION (also known as DNS):

DNS is configured in /etc/resolv.conf . This file is dynamically generated by Network Manager, so any changes you make to it will be lost on reboot(assuming you didn't change the hosts order in /etc/nsswitch.conf). Editing this file is a nice runtime way of testing if a dns nameserver is working.
Again, /etc/hosts will win if there is a conflict with it and the DNS in this file.

/etc/nsswitch.conf is where you can specify which order the configuration files will be processed in.
Ex: 'hosts: files dns myhostname'	(means that first /etc/hosts will be used, then the DNS in /etc/resolv.conf, then the hostname will be used [for local resolution])
So, if you do not want /etc/hosts to override the DNS, you can change the order here.

---

USING COMMON NETWORK TOOLS TO VERIFY THAT YOUR NETWORK IS WORKING:

ping 192.168.4.245	(to test the responsiveness of other hosts)(you can pay attention to the stability of the 'round trip times' at the end of each line, and also their actual speed in miliseconds [lower is better].

dig hostname.com	(is for very fine DNS relay issues)(you can use it to do a hostname lookup)(shows you a detailed status report, including which network it is on, the resolved ip address, which DNS server has provided the resolution of that ip address [so it shows you that the DNS server on that network is functional. This can be useful because more than one DNS nameserver can be specified, so you can find out of the first is not responsive and the second is picking up the slack for it], query time, errors)(a status of NXDOMAIN means No Such Domain. This is usually an error on your side, because since DNS is a worldwide heirarchy, there should never be a situation where it actually disappears.)

These two 'ping' and 'dig' are the most commonly used utilities. Most of the others will not be used. There is also 'nmap'.

nmap provides information about available ports. Be careful when using nmap, because some owners of networks consider nmap a hacking utility, so don't use nmap on ports that aren't yours or you could get into trouble.

----

CONFIGURING THE SSH SERVICE:

vim /etc/ssh/sshd_config (server config) and ssh_config (client config)

sshd_config	(by default, is listening on port 22, but you can modify that for security. PermitRootLogin is set to 'yes' by default, it's a good idea to change it to 'no' if you are not securely behind a firewall and your machine ca be accessed through the internet. PasswordAuthentication is also set to 'yes' by default to make things convenient, but it's much safer to set it to 'no' so that public/private keys are required to login to your ssh server.)

ssh_config	(can set options for every client [each client can have their own file for more config] on your system, like 'ForwardX11') 

---

STARTING AND ENABLING THE SSH SERVICE:

systemctl status/stop/start/restart/disable/enable sshd

You will usually need to restart any service whose configuration file you have configures, because these changes are, in general, not picked up automatically.

---

USING SSH TO CONNECT TO SSH:

ping 192.168.4.245	(just to verify the availibility)
ssh 192.168.4.240	(just use the address of the machine you want to connect to. If you do not specify a username here, it will try to connect using your current user account.)
ssh student@192.168.4.240	(to log in as user 'student'. You will be prompted for your password. Then it will be as if you are sitting at that machine itself.)
ssh -X student@192.168.4.240	(to launch a graphical application. The first time you do this, it will give you a warning about the file .Xauthority not existing, but then anytime you launch graphical applications from the commandline it will allow you to do so and launch them.)
When you are finished, you can type 'exit' and then ctrl-c to get out.

When you use ssh, configuration files are created in ~/.ssh/. First, it will be a file named known_hosts, in which there is the remote hostname/ip address and its key fingerprint. The next time we use that remotehost, the remotehost will send us its public key and it will be verified against what we have in this file on our local machine. If it doesn't match, then the connection will be refused because of suspicion.

ANOTHER WAY TO CONNECT TO A MACHINE WITH SSH IS BY USING SSH-KEYS:

ssh-keygen	(to first generate a private/public keypair)
ssh-copy-id 192.168.4.240	(to copy your public key to the specified remote host)

Now, the next time you login to the remote host, it will use the token that you automatically generate which has been encrypted by your private key, it will use your public key to decrypt it, and then you will be authenticated without having to type your password over the net. This makes using ssh much more convenient for you.

----

USING SCP TO SECURELY COPY FILES OVER THE NETWORK:

scp /etc/hosts 192.168.2.240:/tmp	(will copy the file /etc/hosts to the /tmp/ directory in the remote hosts's machine)
scp 192.168.2.240:/tmp/hosts /tmp	(will copy the file /hosts from the remote host to your machine)

scp is a convenient and easy-to-use utility that is a lot like 'cp'. Many of 'cp's options are supported by 'scp' as well. The only major difference is that you will need to identify the remotehost followed by ':' and the path.

----

FILE SYNCHRONIZATION WITH RSYNC:

By default, rsync uses ssh, so the rsync command is like a client to ssh. 

rsync -avz /tmp student@192.168.4.240:/home/student/tmp	(to syncronize the entire contents of the /tmp/ directory)

As you can see, the command is pretty similar to the scp command, but a little more efficient if you are planning to syncronize large amounts of data.

----

FIREWALLS:

Firewalls can be run on the network, and also on the server itself. On the server itself is less complicated. Firewalling is taken care of inside the Linux kernel, with the 'netfilter' functionality. 'Forwarding' is really only relevant if your machine is/is being used as a router. 

Firewalling on linux is taken care of by the iptables utility. Iptables creates rules which are processed in the order in which they are found. The iptables command is very rich and complicated, so distributions give us other solutions for firewalling (firewalld on redhat, ufw on ubuntu, and SUSEfirewall) which interface with iptables.

---

FIREWALLD (centos):

firewall-cmd is what you use to use firewalld.
firewall-cmd --list-all	(shows the current configuration)('public' is the default zone. You can use multiple zones, such as one for safe internal network and one for an external network, and change the 'interfaces:' line to put interfaces in the specific zones, so based on the interface the file knows which zone it is in and thus which rules to apply.) (The most common items to use are 'services:' and 'ports:', to define which services and ports you want to allow. In firewalld, a default list of services and ports is provided. You can see that list by typing 'firewall-cmd --get-services'. These services are written in /usr/lib/firewalld/services/. The files therein show the configuration of each service.)
firewall-cmd --add-service samba	(This is only runtime, NOT persistent. Adding '--permanent' to the end makes it persistent.)
firewall-cmd --reload	(only persistent changes ['--permanent' option] will survive this reloading of services.)
firewall-cmd --help | grep add-port
firewall-cmd --add-port 4000-4005/tcp	(will return 'success' if the syntax is correct)(again, '--permanent' to make it persistent)

---

UFW (ubuntu):

Stands for 'Uncomplicated Firewall'.

sudo ufw enable	(to enable it, will also enable it on startups)
sudo ufw status
sudo ufw allow ssh
sudo ufw reject out ssh
sudo ufw delete reject out ssh	(to delete that previous rule)
sudo ufw deny proto tcp from 192.168.4.240 to any port 22
sudo ufw reset	(to reset all rules to the installed defaults)

ufw works with applications, which are basically services.

sudo ufw app list	(shows available applications)
sudo ufw app info OpenSSH	(to show you more info on the specified application)

sudo ufw logging on	(to enable logging [for errors and stuff])

man ufw	(for some nice examples)

---

IPTABLES:

iptables {-A|I} {INPUT|OUTPUT|FORWARD} [-i|o <iface>] [-s|d <ipaddress>] [-p udp|tcp|icmp [--dport|sport nn...]] -j [LOG|ACCEPT|DROP|REJECT]
(-A = append, -I = insert. If you insert, you need a number behind the chain {chain is INPUT,OUTPUT,FORWARD}.) (-j = jump)(DROP will silently drop the packet, REJECT will inform them that it has been rejected. DROP is good externally for security, REJECT is good internally for diagnostics. 'LOG' does not end the jump, so you can LOG and then ACCEPT/DROP/REJECT.)
iptables -A INPUT -s 10.0.0.0/24 -p tcp --dport 22 -j ACCEPT	(an example)

iptables -L	(listing traffic in the INPUT|OUTPUT|FORWARD chains)

---

CONFIGURING FIREWALL ON centos USING IPTABLES:

This example sets up SSH communication from scratch.

systemctl stop firewalld
iptables -L
iptables -P INPUT DROP	(change the rule from 'ACCEPT' to 'DROP')
iptables -P OUTPUT DROP
iptables -L	(now the system is secure, though not very usable yet)

iptables -A INPUT -i lo -j ACCEPT	('lo' is the loopback interface)
iptables -A OUTPUT -o lo -j ACCEPT	(allows all incoming and outgoing traffic to/from the loopback interface, so that you can ping your local host again)
iptables -L -v	(verbose)

iptables -A INPUT -p tcp --dport 22 -j ACCEPT	(allowing ssh traffic [port 22 on tcp] to get INTO this system)
iptables -A OUTPUT -m state --state ESTABLISHED,RELATED -j ACCEPT	(allowing all answers to get out of the system, but not new traffic. 'state' is a kernel module that traces packets, and if they are part of an established communication or related to an established communication, then it will apply the chosen jump to it)

iptables -A OUTPUT -p tcp --dport 22 -j ACCEPT	(allow all traffic outgoing from port 22)
iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT	(so that answers are allowed to get back into this system)

Now, to allow outgoing web traffic:

iptables -L -v
iptables -A OUTPUT -p tcp --dport 80 -j ACCEPT	(http is on port 80, tcp)
iptables -A OUTPUT -p udp --dport 53 -j ACCEPT	(53 on udp is the DNS port, otherwise when you try to acces the web you will get a failure to resolve DNS)

iptables-save > /etc/sysconfig/iptables	(to make it persistent, otherwise all these changes will be lost on reboot. /etc/sysconfig/iptables is for centos, for ubuntu it will be a different location)(iptables-save without a redirect just writes to standard output, which is not very useful.)

systemctl status/enable iptables	(if necessary)

---

iptables -A INPUT -p icmp -j ACCEPT	(to allow the 'ping' command in. 'ping' uses the icmp protocol)
iptables -A OUTPUT -p icmp -j ACCEPT
iptables-save > /etc/sysconfig/iptables

----

RUNNING DOCKER CONTAINERS:

systemctl enable --now docker
systemctl status docker

docker search nginx	(to search different nginx images that are offered by docker)
	(in the LFCS exam, make sure that you are working with the official images [will be labelled OFFICIAL in title, usually with the most 'stars'])
docker pull nginx	(download that image)

Docker uses a layered filesystem. Each of the components that is downloaded (by pull) is a different layer.

docker images	(to see locally available [pulled] images, you can also use the option '--all' if there are images you can't see for whatever reason)

docker run --restart-always nginx	(the '--restart-always' option after 'run' is a good idea)

***NOTE: We are running this container as root, which is okay. If you want to run it as non-root, you need to open another terminal and do:
		grep docker /etc/group	(to check if it already exists, though it will probably not, only 'dockerroot')
		groupadd docker
		usermod -aG docker student	('student' being the username)
***NOTE END_

docker ps	(shows you the container ID, its logical name [to make it easier for you to manage]  and how long ago it was started)

docker exec -it wonderful_hopper /bin/bash	('-it' is interactive, 'wonderful_hopper' is the container's logical name, /bin/bash is the command that you want to execute inside that already running container while it is running)

Once you are in the container, as the above command has placed you, if you try to use a lot of commands you might get 'command not found', because containers by nature are a very limited environment.

---

USING STORAGE WITH DOCKER CONTAINERS:

Containers by default are ephemeral (if you set it down, it's all gone), and so is storage. Storage can be mounted in a Docker container, but that ties the container to the host where the storage is accessible. The 'bind mount' mechanism is used to do this.

?First, use the 'bind mount' mechanism to mount the storage to the container. Then,

setenforce 0	(to disable SELinux)
docker run --rm -ti -v /data:/data ubuntu:latest /bin/bash	('-v' executes a bind mount, making sure that the storage is going to be mounted)

----

LXC CONTAINERS:

Namespaces provide isolation, and provide the foundation for containers.
mount = isolation at the filesystem level
IPC = provides process-level isolation
UTS (Unix TimeSharing) = provides unique identifiers in containers
PID = ensures that each container runs its own PID table
User = allows processes to run as root within a container, but not outside of it
Network = sets up isolated networking for a container

---

RUNNING AN LXC CONTAINER:

systemctl disable --now docker	(it is not a good idea to run a docker daemon next to an lxc process on the same computer)

Now install lxc and its dependencies.

lxc-checkconfig	(checks to see if everything you need to run lxc containers is available)

/usr/share/lxc/	(contains templates, which are shell scripts which set up specific environments, i.e. 'lxc-debian')

lxc-create -t download -n lxc-container1	(opens an interactve prompt, asking you which distro you want to use)
	(in the demo, this caused an error/bug, so instead we did the next command, which ran without prompting)
lxc-create -t centos -n lxc-centos1	(creates the container without prompting you)

cat /var/lib/lxc/lxc-centos1/tmp_root_pass	(shows you the temporary root password)

lxs-ls -f	(shows what lxc containers you have and in what state [ours is currently stopped])

lxc-start -n lxc-centos1 -d	('-n' is name, '-d' is to run it as a daemon)

lxc-info -n lxc-centos1		(to get some basic information about the container)

lxc-top	(for CPU/Mem/IO usage for each/all containers)

lxc-attach -n lxc-centos1	(to 'attach' yourself to the container, basically it is now as if you are inside the container running as a user)
exit	(to leave the container)

lxc-stop --name lxc-centos1	(to stop the container)

lxc-autostart, to autostart managed containers, will be on the exam. Check its manpage.

***CONFLICTING INFO: NEWEST INFO SUGGEST THEY ARE USING DOCKER INSTEAD OF LXC IN THE EXAM NOW.

----
		
RAID:

lsblk	(to show the currently existing block devices)

apt install mdadm

mdadm --create /dev/md0 --level=1 --raid-devices=2 /dev/sd[bc]	(--level is RAIDlevel)(you can use either paritions or entire drives at the end)

lsblk	(to check if it took effect)

reboot	(just to make sure they exist properly. If using whole devices and not paritions, you don't need to bother with fdisk during creation.)

mkfs.xfs /dev/md0

mkdir /raid

vim /etc/fstab
	/dev/md0	/raid	xfs	defaults	0 0

mount -a	(to mount everything that fstab contains)

***NOTE: There is also an option to generate a config file with the madm command, but it's not needed so we can keep things simple.

----

RAID is about redundancy, NOT flexibility. If you want flexibility, you should use LVM.

----

LVM:
	The volume group is the heart of LVM. It is made of Physical volumes (which can be either disks, paritions or even raid devices), and can be cut into physical volumes which are then formatted with the desired filesystem.

First step,1) you must use fdisk to create a partition with the type '8e', if you want to use partitions instead of complete disks as physical volumes, because that is the special filesystem which LVM uses. In the exam, it is likely that you will have to use partitions.
	2) pvcreate, vgcreate, lvcreate, mkfs, mount fstab

***NOTE: In the exam, they are looking for persistancy, meaning that anything you do must survive a reboot, otherwise it is wrong.

---

LVM live example:

fdisk /dev/sdd
	m (for help), n (create new partition), p (primary partition), 1 (parition number), enter (default first sector), +2G (last sector, which is size), p (to doublecheck that this is applied), t (to change the partition type), 8e (hex code for Linux LVM), p (doublecheck again), w (write)
lsblk	(to verify)
pvcreate /dev/sdd1	(create a physical volume from /dev/sdd1)
pvs	(to verify)
man vgcreate	(has a good example)
vgcreate -s 16M vgdata /dev/sdd1	('-s 16M' means the volume can be extended by multiples of 16 megabytes. It is 4M by default, and usually works well, but if you have problems you can change it, for example, to 16M)
vgs	(to verify, or 'vgdisplay vgdata' for more details [a way to verify you have set the physical extend size successfully])
lvcreate -n lvdata -L 1G /dev/vgdata	(the simplest way to do things. '-n' = name, '-L' = size.)
lvs	(dblcheck)
vgs	(to see how much space you still have available in the volume group)(in the volume group, one pysical extent [16M in this case] is used for metadata, so it will be lost from the free space)
mkfs.ext4 /dev/vgdata/lvdata	(to make it mountable)
vim /etc/fstab
	/dev/vgdata/lvdata	/data	ext4	defaults	0 0
('mount -a' now will show that it fails to mount, because we haven't actually created the mountpoint directory yet. This is a nice way to check.)
mkdir /data
mount -a	(this time it works flawlessly and will survive reboots)

---

partprobe	(informs the OS about parition table changes. Used when you get a 'device busy' error when writing changes in fdisk. This means that the parition was indeed successdully written, but won't be seen by lsblk because the kernel partition table [/proc/partitions] doesn't see it yet. You can update the kernel partition table by running the partprobe command. On a new device [other physical drive], it is never needed to run 'partprobe'.)

lvresize can shrink ext4, but NOT xfs, because xfs does NOT support shrinking, while ext4 does.
lvextend can be used to just extend and not shrink.

lvextend -r -L +2G /dev/vgdata/lvdata	(automatically takes the space from the volume group, gives an error if not enough space. In that case, go back to 'fdisk /dev/sdd' and create a new parition to add as a physical volume, then add that to the volume group [vgextend /dev/vgdata /dev/sdd2] [if at this point you forgot to create a physical volume with pvcreate, vgcreate will create it for you automatically :) ], then run the 'lvextend -r -L +2G /dev/vgdata/lvdata' command. '-r' automatically resizes the device, so that you don't have a 'small filesystem on a bigger device' problem.)

If you really screwed up and forgot the '-r', then you have to manually use 'resize2fs' (for ext filesystems) or xfs_growfs (for xfs). 'resize2fs' allows you to both shrink and grow the filesystem, while 'xfs_growfs' only allows you to grow it.

resize2fs /dev/vgdata/lvdata	(will automatically adjust it for you, no further info is needed)

echo $(( 128 * 16 ))	(this is the bash calculator. Handy for checking sizes from the commandline)

----

'find' looks for exact match only, not patterns. If you want partial, you can do 'find / -name *hosts'. Basically, using wildcards the way you would use them with the 'ls' command.

find / -perm /4000	(to find all files which have SUID)

find / -perm /4000 -exec ls -l {} \;	(-4000 would be ONLY exact match, while /4000 means ANY file with a SUID)

----

***Regular expressions are not really needed in the LFCS exam, I wouldn't focus on that. They only ask you for basic regular expressions. It's my philosophy of life to avoid regular expressions whenever you can. (sandervanvught)

Some items on the exam are more important than others. The most important items are the ones which relate to storage, containers, and permissions. Complicated questions are weighted more heavily in terms for points than easy questions, so he recommends you actually focus on the complicated questions.

There is not too much about service configuration. Most of the questions about service configuration is about managing and configuring containers. 

----

CRONTABS:

systemctl status crond	(should be automatically activated on your server. crond makes sure it is activated automatically.)

Do NOT use the generic cron file (vim /etc/crontab) because it is a system-maintained file and there are more elegant solutions to maintain your configuration. The only nice thing about the cron file is that you can read the meanings of each star if you forget them.

crontab -e	(is a nice and elegant way or creating a cronfile for the current user.) (55 8 * * 1-5 poweroff shuts off the computer at 8:55 am on workdays [mon-fri])

Most linux distros ONLY allow the root user to run cronjobs decently.? There is 'cron.allow' and 'cron.delay' but these are painful and you will not need it in the exam anyway. 

Another nice way to run cronjobs is in the /etc/cron.d/ and /etc/cron.{hourly|daily|weekly|monthly} directories.
In /etc/cron.d/, you can drop files in there where the filename is the name of the job, and the contents are just like any other cron file.
*/10 * * * * root /usr/lib64/sa/sa1 1 1	('*/10' means once every ten minutes, run as root user, followed by the command and its parameters if any)
However, in the hourly|daily|weekly|monthly directories, they are not cron files but just regular scripts (bash or other) to be run. anacron is what runs the scripts in these directories. The funny thing about anacron is that it does not use exact times, but it will do the jobs every hour or whenever, depending on the directory.

I recommend using crontab -e, because it is the most elegant way and connects it to your current user. I would not recommend dropping files into the cron.d directory because that was meant to be used automatically by rpm files, so you drop cron files that come from rpms in there.

----

visudo	(most of the important stuff is near the end of the file) 
usermod -aG wheel student	(logout and login to apply)(now 'student' can do 'sudo' without being prompted for the password)

----

diff file1 file2	(in the exam you might need to compare files in a directory)
diff /dira/ /dirb/	(also works on directories, to show what files are lacking and extra between them)

----

grep '^root' /etc/passwd > outputfile.txt	(to extract that line and copy it into a new file)('^root' means lines starting with the word 'root')

grep -w the .bashrc	(-w looks for a specific word, NOT a pattern within a word)

grep xmonad ./* --exclude-dir=xpm	(to exclude multiple directories, do --exclude-dir={xpm,examples} )

grep [aeiou] filename.txt (grep for any line that contains a vowel)([x-z])

grep -A 3 -i word file.txt	('-A 3' is grep the line that contains the pattern, plus the next 3 lines after it)('-B 3' is 3 lines before)('-C 3' is 3 lines before AND 3 lines after)(-H = show the filename along with the matching lines, -n = linenumber, -c = count the number of occurences) 

----

"apt-mark hold packagename " to pin a package and "apt-mark unhold *packagename*" to unpin. (can use to prevent snapd from reinstalling itself.)

it is not a dependency of gnome-software-plugin-flatpak, it is a dependency of GNOME SOFTWARE on Ubuntu. When DT installed the Flatpak plugin, gnome-software was pulled as a dependency, also pulling the plugin for Snaps and consequently Snapd. This could have been avoided if he had used "sudo apt install gnome-software gnome-software-plugin-flatpak --no-install-recommends"

----

warning: be sure to add `/home/mgtow1/.cargo/bin` to your PATH to be able to run the installed binaries

----

TIME MANAGEMENT:

hwclock (BIOS)(statring point for time on the computer)
When you boot, the system clock is set based on the hardware clock.
The hwclock by default is often set by UTC (Universal Time Coordinate, is a time that is the same no matter where you are on the planet.
TZ is linux's timezone variable to calculate the offset to UTC. TZ contains daylight savings info as well, so if daylight savings changes, it is automatically applied to the clock.
Once the system has booted, systemclock is all that counts, and your OS will never look at the hwclock again. That means a difference can occur if either your hwclock or systemclock is unreliable, and that is why you want to synchronize time.
NTP is used to synchronize time. The NTP server makes sure that the systemclock gets updated. 

Commands used to manage time:
date = used to set the system clock
hwclock = used to set the hardware clock, and can also synchronize the system clock with the hardware clock, which can be very useful if you have an unreliable hardware clock. On bootup, the first thing your system does is read the hardware clock, and if your hardware clock is off it can cause problems when you try to synchronize with the NTP, so being able to synchronize your hardware clock to the system clock can be useful in these cases.
ntpd, chronyd = services that the NTP goes through.
If NTP is involved, then you should make sure that the difference between the system time and NTP time is no more than 10 minutes, because if the difference is greater than 10 minutes then the system clock will consider the NTP time incorrect and will refuse to synchronize. 

-

hwclock
date	(system clock, you can eyeball the difference between it and the hwclock)
date -s 19:15	(to set the time to 19:15)
hwclock --systohc	(synchronizes the systemtime to the hardware time)
hwclock --hctosys	(synchronizes the hardwaretime to the systemtime)
timedatectl --help
ubuntu doesn't really have a convenient utility to change timezones, other than timedatectl (which was mainly for centos)

-

NTP:

Stratum defines the importance of an NTP timeserver. Stratum 0 = most important, most reliable, usually atomic clocks over the internet.
If a server synchonizes with a stratum 0, it is given the rank of stratum 1, and if a client in turn syncronizes with that server, it gets a ranking of stratum 2. By requesting the stratum of a server, you know how reliable it is. An NTP client normally has a choice of a few different servers, and it will do business with the most reliable server that it can find. 
Stratum 10 is usually used for local clock. That means it has some kind of time sychronization, but it is not something that should really be used. Stratum 16 means that this server isn't in business yet, and is unavailable as a time provider.

In time synchronization, if there is a difference between the client and the server then it is normally adjusted slowly, i.e. the client will slowly slow down its clock until it matches the time of the server, and this will take a while. If you don't like that, you can use 'iburst' option to IMMEDIATELY set the time correctly if there is a difference between the client and the server. 

-

ntpd:

chrony is more accurate at a nanosecond level. If you don't care about nanoseconds, then ntpd is good enough.

ps aux | grep ntp	(to see whether ntpd is running)

/etc/ntp.conf	('fudge' synchronizes with the hardware clock)('fudge 127.127.1.0 stratum 10' [you need to identify the stratum for this. By default, internet time will be preferred but if unavailable it will use yours.])

ntpq -p	(to print info on who you are currently synchronized with)

---

chrony:

systemctl status chronyd	(partial matches, such as 'chrony' do NOT work and will NOT show chronyd)
/etc/chrony.conf	(just like the ntp config file, you can see a list of servers here that you are synchronizing with)(if you want to add anything to the chrony synchronization, you can do it here)

chronyc sources	(lists the current servers configured and their status, same as 'ntpq -p'
chronyc tracking	(for more detail, to display tracking info)(The last line, 'Leap status', can show you that you are 'Not synchronized'. In such case, you probably need to make sure that your filewall is open for time traffic, by:)

iptables -A INPUT -p udp --dport 123 -j ACCEPT	(to allow incoming time traffic) 
iptables -A OUTPUT -p udp --dport 123 -j ACCEPT	(to allow outgoing time traffic)
iptables-save > /etc/sysconfig/iptables	(for centos. Check to see the equivalent for Ubuntu)

Now, 'Leap status' will show as 'Normal'.

By default, time traffic uses udp and not tcp.

---

Lab: Set up 2 VMs (a server and a client), and make an ntp between them.

hwclock
date	(eyeball it to make sure they are close)
systemctl status chronyd	(to see that it is currently running)
chronyc tracking	(see that leap status is normal and which stratum it is using, meaning that it is synchronized)
chronyc sources	(to find out what it is synchronized with)
vim /etc/chrony.conf	(to check the configuration, you shouldn't need to chagne anything)
iptables -L -v	(to make sure that ntp traffic is allowed in input and output of the firewall [will show as 'ACCEPT' in the third field and 'udp dpt:ntp' in the last field])

***ALL OF THE ABOVE WAS DONE ON THE SERVER. NOW, WE LOG IN TO THE CLIENT:

vim /etc/ntp.conf	(comment out the first set of external servers by using #, then add the new server where it says in the comments 'Udisciplined Local Clock', on top of the 'server 127.127.1.0 #Local Clock (LCL)' and 'fudge 127.127.1.0 stratum 5 #LCL is unsynchronized' lines. We will add the line: 'server 192.168.4.240 iburst', which is the ip address of our server machine and any options we prefer, such as 'iburst')
systemctl restart ntpd	(to restart the ntp process, allowing it to take in the changes we made to the config file)
ntpq -p	(to display information about what's happening)('.INIT.' shows that the server is currently initialized. It may take a couple of seconds to do it. It will still have '.LOCL.' [the localclock] as a fallback. Initially, 'st' stratum will show as '16', which is the unreachable status, but in a minute it should change to the approproate stratum.)

----

PROCESSES:

kernel, init 1 (systemd), a lot of kernel threads started by systemd (shown in [] square brackets), crond, rsyslog, httpd. Later down the line you will see a bash shell. Bash is not a direct child of systemd, but we don't really care what it's a child of. In the bash shell you run commands.
Processes started by the user in a shell environment, are also manageable by the user as 'jobs'.
A thread is a task/subtask that can be started by an individual process. In other words, a thread is a subtask that's part of one single process. Some processes use threads, other processes just spawn different processes. This is important for an admin to know, because managing threads is much more complicated than managing processes. 

-

A user managing his own shell jobs:

ctrl-z	(pause a running job and put it in the background, still paused)
bg	(to run a paused job as a background task)
&	(denotes a background job)('sleep 700 &')
fg	(by itself will move the last job to the foreground)
fg 1	(moves job number 1 to the foreground)
ctrl-c	(easy way to cancel out of a job, the job disappears for good [is shut down])

-
 
MANAGING PROCESSES:

top is the most important tool to manage processes.

top (last minute, 5 minutes, 15 minutes)(press '1' to change view from summary of cpus to 1 line per each individual cpu core)
	('stopped' tasks mean tasks which have been stopped using ctrl-z. 'zombie' is a processes that has lost communication with its parent process, and that's normally bad. If you see zombie processes, you need to do something about it.)('sy' systemspace usage comes about when programs are talking directly do linux devices, such as with the 'dd' command.)('ni', nice, shows the processes for which the bice level has been adjusted. 'wa' means waiting for I/O, it is an important one. If you see 'wa', that typically means that your system is waiting for hard disk. A high percentage of 'wa' is bad for performance. The last three parameters 'hi''si''st' don't really matter, so I won't waste time discussing them.)
In terms of memory, cache and buffers are more or less similar, and they are used to make your system run smoothly. A certain amount of memory used in buffers and cache can be freed easily, as it is not really needed for your operating system to run. That is what is reflected in 'available memory'.

By default, top sorts processes by cpu usage. 'Time' is the amount of time a processes has been busy(?running). 'S' = status, R for running and S for sleeping.

As far as memory shown in top, there is VIRT, RES, and SHR. VIRT is virtual memory, which is an address space that a process has claimed, but it's just a couple of pointers and is not really used. RES is resident memory, which is the amount of memory a process REALLY is using. SHR is shared memory, which is memory that a process is using but sharing with other processes. 

top -u student	(shows processes that have been started by 'student')

In top, pressing 'f' shows a list of fields. You can select/deselect fields which you highlighted by moving with the arrow keys. You can press 's' to sort by a specific field, and press 'q' to get out of that menu.
pressing 'z' gives you colors. 'W' (uppercase) will write the configuration file '.toprc', and will use those settings every time you open top. 

---

ps	(without any arguments, shows the processes that you as the user are the owner of in the current shell)
ps aux | less	(to show you all processes. Useful for admins.)

kernel threads [] are not ordinary processes and you cannot manage them normally.

PIDs are normally allocated at the time that a processes is started, so they say something about the starting order.
 
ps -ef	(shows extra info as compared to 'ps aux', including the PPID [parent PID, the one responsible for spawning{starting} that process. Thus you can see that '[kthreadd]' is an important processes responsible for spawning many of the other kernel threads.])   

ps -e -o pid,args --forest | less	(-o = output options)(forest = clearly displays the relation between parent and child processes)
					(args shows which arguments have been started with each processes)
ps aux --sort pmem	(shows the processes using the largest amount of memory last)

----

CHANGING PROCESS PRIORITY:

In top, 'PR' shows priority. The same number among processes means they get an equal share of cpu time. Here, 'rt' instead of a number means 'realtime', and these are typically processes that are a part of the kernel.
As an admin, you can change processes priority using the 'nice' command. 
In top, press 'r' to get a renice prompt, which will ask you for the PID to renice. By default, it will take the first PID in top's list of active processes. Next, it will ask for the value to renice to (-20 to 19). If you are root, you can nice/renice processes with negative as well as positive values. Ordrinary users can only use positive values. If you renice to -5, then the priority of the process changes to 15. If you use renice/nice and change the value, that value will be added/subtracted from the current priotity for the new priority. You can also observe it getting more/less CPU cycles (usage). I recommend you change nice values using increments of 5. Don't immediately jump to -20 on a process because there won't be any time left for other processes.

nice -n 5 <command>	(will start the command with the wanted nice value)

'renice' works pretty similarly to the nice command. 

renice -n 5 14053	(using PID)

You should only use 'nice' and 'renice' on specific occasions, not out of habit. If you have structural problems of processes not getting enough attention, there are better solutions like 'systemd cgroups' for example, which allows you to define in detail which process priorities will be used by specific processes.

----

MANAGING PROCESSES WITH SIGNALS:

man 7 signal	(to read about signals)

The 'kill' command is what sends signals to processes. Signals are like instructions.

SIGQUIT (signal 3) is like using ctrl-c to stop processes from the keyboard.

The most commonly used signals are SIGTERM (15), SIGKILL (9), SIGHUP (1) and SIGQUIT (3).

SIGTERM cannot be ignored per se, but since it allows the processes to shut down everything that it is doing properly, it may take a long time. And there may even be conditions which prevent the processes from shutting down properly, and that is why we have SIGKILL. But we risk damaging things when we use SIGKILL, since it doesn't allow resources to be properly managed. 

In top, use 'k' to kill by PID. It will prompt for PID, then SIG, first suggesting using SIGTERM, which if used on top would be the same as quitting by pressing 'q'. 

In most cases, you will be able to avoid using SIGKILL.

'killall' command can be used on names, 'kill' can be used on PIDs. Some distros have alternative solutions like 'pkill'. 

kill 14053
killall dd

In linux generally, if a command doesn't show any output that means it is successful. Suspended background tasks can be killed with SIGKILL cleanly because they were not using any resources at that moment.

kill $(pidof dd)

----

MANAGING SOFTWARE:

Source files are typically written in the C programming language. 

INSTALLING SOURCE CODE:

Go to sourceforge.net, download any package (for example nmap gui). The file will have the extension .src.tar.gz . 
tar xvf file.scr.tar.gz
When you work with sourcefiles, there is usually a Makefile, which contains instructions for the compilation of the source file. The README.txt file may contain instructions on what to do. If Makefile is present, the command 'make' with no arguments will typically run it. But first:
You will often find a script ./configure . If present, run that script.
Next, you will run 'make', which does the actual compilation where the source files are compiled and made into runnable program code.
Then, you do 'make install'. Typically, you will run it as 'su -c make install' as root, because it need authority to place the binaries in the correct locations. All the stages up until this point will typically be run as a normal, non-root user. 

If the source code has a dependency that is not present in the same location as the package itself (sourceforge), it can be an indication of problems further down the line.

The subdirectory /src in the extracted package is where the program code itself is usually kept.

---

INSTALLING SOFTWARE FROM TARBALLS:

tar xvf <tar.gzfile> -C /tmp	('-C' extracts to the specified directory)
gunzip on a tar.gz file leaves you with a .tar file (archive)
tar tvf <tararchive.tar> | less	(to check what's inside it first, list its contents)

CREATING TARBALLS:

tar cvf etc.tar /etc	(c = create the archive, z=gzip,j=bzip2,J=xz, v=verbose, f=output as a file)
gzip etc.tar	(to compress the newly created archive)

By default, the entire path including the original directory is included in the tar archive. This is good, because it prevents it from being a 'tarbomb'.

The tar -u option will only append newer files to the archive. Useful for backup. '-p' perserves permission, which is the default for the root user but not for normal users, so you can use this as a normal user to preserve the original permissions, so that you're not stuck will all those files just belonging the the normal user. 
 
---

LIBRARIES:

ldd /usr/bin/passwd	(shows all the libraried that this command needs. Most of the libraries are files, and these files are in the library cache. These files add additional functionality. 'libc' is the generic linux c library, you will see it a lot.)

Commands usually know which libraries to use thanks to the library cache. On occasion, the library cache fails to update automatically so you can run 'ldconfig' with no arguments to update it. 

/etc/ld.so.cache	(contains all libraries that are included in this system)	.conf (by default just tells linux to look in the .conf.d/ directory)	.conf.d/ 

Normally library dependencies are dealt with automatically when packages are installed, and ldconfig is started automatically after installation. 

ldd $(which ls)	(can be used to find out of a specific library is present)

---

DPKG:

dpkg --get-selections	(view all installed packages)
dpkg -L vim	(list all the files that are in the package vim)
dpkg -p vim	(show more info on that package)
dpkg -S /usr/bin/eject	(see from which package a file opens)
dpkg -S eject	(same thing, it is in PATH)

---

APT:

/etc/apt/sources.list	(is where apt-cache repositories are listed)

apt-cache show <packagename>	(shows more info on a package)
apt-cache depends <packagename>	(shows all dependencies of a package)
apt-cache rdepends <packagename>	(shows all packages which depend on specified package)(rdepends = reverse depends)
apt-cache stats	(shows statistics)

apt is completely backwards compatible with apt-get, so you can substitute freely.

apt-get update	(synchronizes package list with /etc/apt/sources.list)

apt-get dist-upgrade uses a smarter dependency resolution method than apt-get upgrade, differenciating between more important and less important packages, where less important packages may be dropped. Good for upgrading an entire distribution.

apt-get check	(checks currently installed packages for broken installation)
apt-get clean	(cleans the package cache)

apt list --installed	(shows all installed packages)

----

SCHEDULING TASKS:

3 solutions: 1 - at (used to run a task once at a specific time)
		2- crond (service that works with a lot of configuration files to figure out if anything needs to happen)
		3- systemd timers (basically the same things that you can do with cron

crond is the most important of these.

Nobody modifies /etc/crontab directly anymore. Instead, you can modify the contents of cron.d/ .
Another alternative is 'crontab -e' to create userspecific cron jobs, which will be stored in an environment specific to the user (and maybe -u to specify the username).

anacron is a helper service that manages the shell scripts in the cron.daily|weekly|monthly|hourly directories.
anacron is nice in that if you miss an appointed time (due to restart/shutdown), anacron will make sure that the job is performed a short time after.

In cron.d/, put a file that is named after the job itself. The contents of the file use the same format as a regular cronjob (the time fields, user(optional) and command.

---

SYSTEMD TIMERS:

cd /usr/lib/systemd/system	(This is where systemd creates its so-called 'unit files', which contain services and other things as well.)
ls *timer	(to see installed timers)

systemctl status fstrim.timer
systemctl start fstrim.timer
systemctl enable fstrim.timer

-

vim fstrim.timer
change 'OnCalendar=weekly' to 'OnCalendar=daily'

After modifying anything in systemd configuration files, you need to run:

systemctl daemon-reload

Then, you can:
systemctl restart fstrim.timer

And verify with:
systemctl status fstrim.timer

---

SCHEDULING TASKS WITH AT:

systemctl status atd

at 11:00	(will open the at prompt for you to type your command, such as 'mail -s hello root < .' [sends a mail with subject hello to the root user. The mail command will wait for the '.' on a seperate line before it starts sending the email, which is why we used that redirection. You can type multiple commands if you want to, and once you're finished you can end it with ctrl-d. 'teatime' is 4pm.)

at teatime tomorrow	(will also work, will execute at teatime tomorrow)
at 4pm + 3days (to run a job at 4pm three days from now)
at now + 5 minutes	(to run a job 5 minutes from now)

atq	(to see which jobs will be running)
atrm <jobnumber> (to remove jobs)

---

crontab -u student -e
4 * * * * logger hello

----

CONFIGURING LOGGING:

systemd-journald is a runtime log, which means that by default it is not stored anywhere, and will be gone after reboot.

syslog saves files to /var/log/.
rsyslog is completely backwards compatible with syslog.
logrotate is a helper service that makes sure that logfiles don't grow too big. With logrotate you can monitor these files and rotate them the moment they get too big or too old or whatever.

---

WORKING WITH SYSTEMD-JOURNALD

The easiest way to access this info is 'systemctl status <servicename>'
journalctl	(another way to look at the filesystem journal. It is stored in memory, is timestamped, and will be truncated automatically if it grows too big. Uses 'less', so you can press 'G' to go to the end.)

To make journalctl persistent, all you need to do is to go to the /var/log/ directory, and create a subdirectory there called 'journal'. That's it. Then you can reboot to double check that new directories containing log files are present there.

/etc/systemd/journald.conf	(parameters which are interesting to look at but not that important)

---

RSYSLOGD:

facility.priority destination	(destination can be a log file or :<users>)(these are the rules found in /etc/rsyslog.conf)('-/<destination>/' makes sure that messages are referenced and not written in realtime, i.e. it uses buffers instead. This is handy for making sure that email messages are not lost in the event of a crash.) ('*.emerg :omusrmsg:*' makes sure that all users recieve a message in the event of an emergency)

/etc/rsyslog.conf rsyslog.d/

in rsyslog.conf, '$ModLoad <module>' extends the functionality, for example 'imjournal' provides access to systemd journal, and 'imuxsock' provides support for local system logging via the 'logger' command.)

Some modern services don't have their own facility. For these, you can configure these services with the facilities local0 to local7. 

Anytime you make changes to the rsyslog.conf file, do:
systemctl restart rsyslog

Test with:
logger -p crit CRITICAL SITUATION	(if a critical or higher priority event occurs [-p is priority], a log will be created in the specified destination)

---

LOGROTATE:

journald automatically removes files that have gotten too big. But in rsyslog, you use 'logrotate' to do this.

logrotate is a cron job. It is in the file /etc/cron.daily/logrotate . 

In /etc/logrotate.conf you can specify how often the rotation will happen, actions to be taken (such as creating new files and how to name those files), what to rotate (you can create seperate config files for log files in /etc/logrotate.d/), and what exceptions there are for files on a different time schedule. 

example of setting *.crit /var/log/crit to rotate daily in /etc/logrotate.conf:

/var/log/crit {
	daily
	rotate 300	(300 is the number of times you want to rotate it)
}

----

BASIC KERNEL MANAGEMENT:

syscalls are a specific set of instructions that can be used to have the kernel do something. Users can generate system calls by typing commands into bash. These syscalls will be interpreted by the kernel and transformed into action. Applications also generate syscalls. 

/proc is the kernel interface. You can monitor and optimize the kernel through the /proc interface.

-

lsmod | less	(shows the loaded kernel modules) (The 'Used by' field shows how often a module is called upon by other kernel modules, meaning that these internal modules also have some dependencies.)

Modules are loaded automatically when they are needed. 

modprobe -r <modulename>	(to unload [remove] a module)

modprobe <modulename>		(will load the module)

modinfo <modulename>		(shows you what a module is doing)

modprobe cdrom autoclose=1	(this allows you to use options. In this case, it allows the cdrom tray to close its door automatically)

The files in /etc/modprobe.d/ will show you some options that can be used with modules.

echo options cdrom autoclose=1 > /etc/modprobe.d/cdron.conf	(to create a file with a new option to be used)

---

OPTIMIZING THE KERNEL THROUGH /PROC

/proc/sys/ contains kernel tunables, which are options which can be turned on or off quickly and easily.

for example, you can:

echo 1 > /proc/sys/net/ipv6/conf/disable_ipv6	(to immediately disable ipv6, until the next reboot)

You cannot use vi to directly edit these files, because vi will put a lock on it. Instead, you have to echo your desired values directly onto the file.

-

To make these changes automatic, you need to write it to the configuration file /etc/sysctl.d/<nameofmodule>.conf 

example: 

echo net.ipv6.conf.all.disable_ipv6 = 1 > ipv6.conf	(the 'net.ipv...' is the path from but not including [after] /proc/sys/ , with '/' being replaced by '.')

-

modprobe is a clever program that automatically unloads modules which are not used and no longer needed by other modules.

---

sysctl -a | grep forward	(to check whether your system can forward ip packets [i.e. that it can be used as a router]. If you see 'net.ipv4.ip_forward = 1' in the output, then this means yes, it can.)

sysctl  is  used to modify kernel parameters at runtime.  The parameters available are those listed under /proc/sys/ .

----

CONFIGURING GRBU2:

If grub does not show up automatically, press escape right after you have started your computer and hardware has been initialized. You can press it repeatedly if needed.

The rescue kernel (usually the 2nd option) loads the kernel with a minimal amount of modules to increase the chances that your system will boot successfully.

Press 'e' in the grub menu to edit the entry. 
insmod <modulename>	(is how grub loads modules)

The important lines are the ones that start with 'linux16' and 'initrd16'.
The 'rhgb' and 'quiet' options hide what is happening when the system is loading, so it can be useful to delete those options. Modifications here are not persistent.
Once you are done editing the options, press ctrl-x to start.

/etc/default/grub is where you can make persistent changes. Once you have made changes here, you need to write it to the grub configuration file in /boot/grub/grub.cfg by using the command 'grub-mkconfig -o /boot/grub/grub.cfg' . Then reboot to verify that the modifications have been applied.

---

TROUBLESHOOTING BOOT:

'e' in grub
To the end of the linux16 line, add 'systemd.unit=rescue.target' to start the OS in rescue mode. Rescue mode is a minimal mode where a minimal amount of services will be loaded. It will drop you to a prompt, where you can fix your problem. Once you have fixed it, you can press ctrl-d to continue booting your system. This rescue mode is the first thing you should try.
By the way, the '0-rescue' entry (usually the second grub option) does not start rescue mode, it only starts a kernel-specific mode, which doesn't affect the number of services started.

systemd.unit=emergency.target	(for when the 'systemd.unit=rescue.target' still loads too many modules)
In this mode, even the filesystem will be mounted as readonly, so you will not be able to create/write files. To change that, type 'mount -o remount,rw /' into the command line. 

echo b > /proq/sysrq-trigger	(not recommended, a nasty way to tell the kernel to crash and reset your system)

Back at the grub linux16 entry, 'rd.break' breaks into the boot procedure just as initramfs has finished loading, meaning that you break into your boot procedure at a very early point. It puts you in a root shell without prompting you for the password, so this is useful incase you don't know the root password. In this mode, the root filesystem is not yet mounted on root. It is instead mounte on /mount/sysroot . So the first thing you have to do is 'chroot /sysroot'. This will set the root of the filesystem to the contents of the /sysroot/ directory. Next, you have to 'mount -o remount,rw /'. Verify that you can now write by 'touch aa'. If you have lost the root password, you can just write 'passwd' and enter a new password. 

---

SYSTEMD:

/usr/lib/systemd/ is systemd's main configuration directory. 

/usr/lib/systemd/system/ contains persistent configurations of your OS. This directory contains different kinds of unit files, such as *.mount, *.service, *.target (target is groups of unit files). Everything in this directory is static, meaning it is not meant to be changed by the administrator.

/etc/systemd/ is where administrator's changes should be made. This is the "dynamic" part, as opposed to the "static" /usr/lib/ .

/etc/systemd/system/ contains those unit files as well. This is where you will make changes that you do not want to be overwritten by system updates. This may involve copying some files from /usr/lib/systemd/system/ to here, and editing/modifying them here.

/run/systemd/ is for everything that had been generated dynamically. The binaries in /usr/lib/systemd/ will work on old configuration files to write new configurations that systemd is going to work with. So you can get a complete overview of everything that systemd is doing here in /run/systemd/system/ . 

---

MANAGING SERVICES WITH SYSTEMD:

systemctl -t help	(gives an overview of all available unit types)

For LFCS, the service unit is the most interesting.
socket units allow systemd to listen to a specific port, and if anything happens on that port it will start the service that is related to that socket.
mount units can be used for automatic mounting.
timer units can be used to start things at a specific time.

ls /usr/lib/systemd/system/*socket	(to see available sockets)

-

systemctl disable sshd	(because 'sshd.socket' conflicts with 'sshd.service')
systemctl enable sshd.socket
systemctl stop sshd
vim /usr/lib/systemd/system/sshd.socket	(in this file, '[Socket]' 'ListenStream=22' means that it will listen for activity on TCP port 22. For UDP, it will be 'ListenDatagram=22', but ssh is TCP-oriented. 

This way, whenever someone tries to ssh in, it will automatically open an ssh connection without needing to actually run the sshd service, because the socket takes care of that all on its own. This is nice in the sense that you can save resources by not having the service running all the time. The unit is only used when needed.

-

Service files consist of 3 different sections: [Unit], [Service], and [Install].

[Unit] contains generic info about the unit including its dependencies. 'After=' means that the service can only be started after these specified units are started. 

[Service] contains the service definition itself. 'ExecStart=' is what systemd is going to start the moment the service is going to be started (usually the service binary itself). 'Restart=on-failure', 'RestartSec=42s' (will restart 42 seconds after failure). 

[Install] is importatnt for the target. 'WantedBy=multi-user.target'

-

systemctl show sshd.service	(shows all the different parameters that can be included in the unit file)

man systemd.directives	(index of configuration directives for unit files)
man systemd.unit	(information about directives that matter in unit files)

systemctl set-property httpd.service MemoryLimit=500M	(only works on active services, otherwise you get an error message)
	(This is persistent. It automatically adds a drop-in file /etc/systemd/system/httpd.service/50-Memory-Limit.conf , containing '[Service]' 'MemoryLimit=524288000'. This will be added to the [Service] section that already exists in the main unit file.)
	(The other way to do this is to manually create that file yourself.)

-

cp usr/lib/systemd/system/rsyslog.service /etc/systemd/system/
vim /etc/systemd/system/rsyslog.service	(modify the line 'Restart=on-failure' to 'Restart=always', and add 'RestartSec=3' to restart after 3 seconds if anything goes wrong.)

systemctl status rsyslog	(the 'Loaded:' line shows 'loaded /usr/lib/...' meaning that it hasn't picked up our modification)
systemctl daemon-reload		(to let it pick up the modification)
systemctl status rsyslog	(now shows 'loaded /etc/systemd/...' so we are using the modified settings)

killall rsyslogd	(to test the restart on failure, wait for a few seconds then type:)
systemctl status rsyslog	(you will find it is loaded again and running like we want it to, with the new configs)

---

WORKING WITH SYSTEMD TARGETS:

A target is just a group of unit files. Groups of unit files can behave in a specific way.  

ls /usr/lib/systemd/system/*target

Some targets can be defined to reach a specific state, whereas for other targets that is not true.

multi-user.target is the default mode that most services start in.
Inside a target [Unit], the 'AllowIsolate=yes' option means that the target can be used as an endpoint.

ls /etc/systemd/system/*wants/*	(shows all 'WantedBy' targets, contains nothing but symbolic links to /usr/lib/systemd/system/*.service for the appropriate service file)

If you include a service in the (for example) multi-user.target, it creates a symbolic link in /etc/systemd/system/*wants/ . 
When the desired target is starting, it reads the contents of the directory that is symlinked to in the /etc/../../*wants directory and makes sure that those symbolic links are processed.
So, if you do not see a *.service link to a specific service here, then that means if you 'systemctl status <servicename>' then that service will show up as 'disabled' and its vendor preset will also say 'disabled'. So when you do 'systemctl enable <servicename>' then it will automatically create a symbolic link in /etc/systemd/system/*wants/*.service to /usr/lib/systemd/system/*.service .
'systemctl disable <servicename>' will remove that same symbolic link. This is how a target knows which services need to be included. The glue between the service and the target is 'systemctl enable <servicename>'. 

systemctl get-default	(shows you the default target, which is usually 'graphical.target')
systemctl set-default multi-user.target	(changes the default target to multi-user.target)

You can even do stupid stuff like 'systemctl set-default printer.target', and it will set it to that, but you shouldn't do that because you should be careful to ONLY set targets that have the 'AllowIsolate=yes' option set in the target's [Unit], otherwise you will cause bugs and crash your system. In this specific case, it will start the printer service and then keep waiting forever, not allowing you to get to a prompt. This is useful if you want to create a linux appliance, but not useful if you want a linux server.
If you screw up like this, then you must reboot then 'e' to edit the grub menu, at the linux16 line add 'systemd.unit=rescue.target' (without the quotes), then when you are at the shell prompt 'systemctl set-default multi-user.target' (or graphical.target) and reboot to verify that all is well.

systemctl list-units	(shows all unit files that systemd is currently using (that have been loaded. It's a complete overview of everything that has happened in more or less the order that they have been started. If anything went wrong it will be marked in red to make it easier to see.  You can also do 'list-sockets' or whatever else.) (add '--all' at the end to also see inactive units)

systemctl list-dependencies httpd.service	(shows everything that needs to be present for httpd.service to be up and running)
	(so the entire road that the system has travelled in order for the webserver to be started)

---

systemctl isolate graphical.target	(you can use 'start' instead of 'isolate' if it's possible to run a target from the current environment. 'systemctl start graphical.target' will work, but it is not as nice as using 'isolate'. graphical.target can be 'start'ed from multi-user.target since it is built on top of it, but if you need to go the other way then you need to use 'isolate', because that will 'isolate' you into the new target that you have defined.)

----

MANAGING SELINUX AND APPARMOR:

Apparmor is used by default on Ubuntu and SUSE, is relatively easy to configure, and can find services that have higher security risks. SELinux, used on Centos by default, is more complicated but is more efficient in securing a linux server. 

-

APPARMOR:

aa-status

apparmor ONLY enforces for applications that have a profile loaded in /sys/kernel/security/apparmor/profile .
The profiles themselves are in /etc/apparmor.d/ . The files are the full command/lib path, but with '.' instead of '/'.

aa-genprof $(which vim)		(to create a profile yourself. You then open up the application in a new terminal, use all of the functionalities which you would use, then press 's' in the genprof prompt to scan for the application events, then prompt you with a rule for eeach event.)
(G)lob is to allow the application to write to the path it is trying to write in [i.e. the /etc/ directory], Glob with (E)xtentions is to only allow it to write into the path [i.e. /etc/ directory] using its specific extension [i.e. .swpx]. After you are finished and save the profile, it will be created as /etc/apparmor.d/bin.vim .

---

SELINUX:

Selinux is either enabled or disabled, and it is done in the kernel, so you MUST REBOOT each time you toggle between enabled and disabled.

If SElinux is enabled, it can be in Enforcing mode, or Permissive mode (permissive mode is easy to troubleshoot in).

If a service stops working, then you put SElinux in permissive mode and it suddenly starts working again then you have proved that the problem is in SElinux. Otherwise, the problem is in the configuration of that specific service itself.

setenforce	(toggles between enforcing and permissive)

Edit the /etc/selinux/config file then reboot to enable/disable selinux (SELINUX=enforcing/disabled, SELINUXTYPE=targeted). But it is better to not touch this file and just use the 'setenforce' command to change the SELINUX mode that is used in oder to keep your system under the protection of SELINUX.

getenforce	(will show you if you are currently in Enforcing mode)

setenforce Permissive	(or 1/0, where 1 is Enforcing)

---

SELINUX CONTEXT LABELS:

ls -lZ	(the 'Z' option is available for many commands to see the SELINUX context labels)

ps Zaux	(shows you a list of all processes with the context type that they are using)

SELINUX mostly focuses on services. For users, it is often easier to put them in unconfined modes (meaning it is not restricted by SELINUX).

netstat -Ztulpen	(context types are also available on ports)

SELINUX rules are created to match source context types to target context types.

-

vim /etc/httpd/conf/httpd.conf	(here you can change the 'DocumentRoute "/<directoryyouwant>"')(You must also change the '<Directory "/<samedirasabove>')
systemctl restart httpd	(for the changes to take effect within the httpd service)
Now apache will serve documents from your newly set document route.
elinks http://localhost	(will automatically serve you the file 'index.html' found in '/web/', which you have set as the directory and created the file in)

However, now we have another problem. Services with one context are not allowed to access directories with a different context, so apache having the context 'httpd_t' is not allowed to access the '/web/' directory because the directory has a context of 'default_t', so we need to change the context of the directory.

man semanage-fcontext	(it has an amazing manpage) 
semanage-fcontext -a -t httpd_sys_content_t "/web(/.*)?"	(to change the context for everything under the '/web' directory)
	(This command will write the new context to the SELINUX policy, but it doesn't apply it to the filesystem until you do:)
restorecon -Rv /web

-

Another example: This is if you want to run the ssh process on a non-default port:

vim /etc/ssh/sshd_config	(change the line '#Port 22' to, for example, 'Port 2022' [or whatever new port you want to use]. In the preceding lines, it is commented that if you want to change this port on an OS that is running SELINUX, you have to tell SELINUX about the change by doing:)
semanage port -a -t ssh_port_t -p tcp 2022
systemctl restart sshd

Then test that it worked by: 'ssh -p 2022 localhost'

---

MANAGING SELINUX BOOLEANS:

getsebool -a	(shows you a list of all booleans currently existing)('| wc -l' to see how many)('|grep <pattern/service>)

setsebool ftpd_anon_write on	(to allow anonymous writes in your file server. If you don't do this, then no matter how you configured the ftp service, SELINUX will not allow anonymous writes in your ftp server.) (This is only runtime. To make it persistent, use the '-P' option, so:)
setsebool -P ftpd_anon_write on		(applied as persistent)

---

TROUBLESHOOTING SELINUX WITH SEALERT:

sealert monitors what is happening on your system. If something is blocked, it will write a message to /var/log/messages .

less /var/log/messages	(then '/' to search for 'semanage', find the appropriate line and copy/paste it to see the exact issue:)
sealert -l 95a15401-1cf2-401d-a53a-06515ff0994d	(the line in question to show us the issue in detail)
The problem here was that it was unable to identify the filetype context. So you can go back and set that, preferably on the whole '/web' directory like you did before and not just on the file itself as it will tell you to do here.

-

sealert is useful if you know what you are doing, but if not you could risk opening up your system to external attacks by using the ausearch ......... command.

----

STORAGE MANAGEMENT:

For GPT, use gdisk, not fdisk. fdisk does have GPT compatibility, however currently its features are not yet fully ready.

---

fdisk MBR:

On modern disks, sectors are 512 bytes by default (fdisk MBR).

Normally, if you use an extended partition you make it your last partition, so everything that won't be available in the extended partition in that case won't be addressable any more.

You must create logical partitions inside the extended partition, otherwise the extended parition is nothing but a blank space.
Logical paritions will be shown as starting and ending within an existing extended parition, so the extended parition will have its own partition number and the logical parition inside it will have a different partition number (can look confusing if you don't pay attention to the start/end blocks).

Use the 'partprobe' command by itself if you get an error message saying that the kernel partition table could not be synchronized because the parition was already in use when you tried to write a new partition table onto it.

---

mkfs.ext4 -b 1024 -L myfs /dev/sdb1
mount /dev/sbd1 /mnt	(to test it from the /mnt directory)
It doesn't complain, meaning that it worked. Use the 'mount' command by itself to double-check.

The 'lost+found' directory can be used by the fsck utility.

umount /mnt	(unmount it so you can create the next filesystem.

cat /proc/partitions	(just to make sure what you have)
Here sdb2 will have only 1 block because it is an extended partition that cannot have data written to it, as opposed to sda5 which will have 512000 blocks as the logical partition we set up in the previous example.

mkfs.xfs -L XFSFS /dev/sdb5		(It is NOT possible to put a filesystem on an extended parition, just on the logical paritions within it)

mount LABEL=XFSFS /mnt	(another way to mount a device, this time by label)
	(you can also use LABEL=<whateverlabelname> in /etc/fstab instead of /dev/sd*#)

mkfs.btrfs -L butter /dev/sdc1

----

mount -a	(will mount everything in /etc/fstab that is not already mounted. If there are errors, it will output them to terminal)

ALTERNATIVELY, THERE IS SYSTEMDMOUNT INSTEAD OF FSTAB:

ls /usr/lib/systemd/system/*mount

To use it, for example (first remove from /etc/fstab and unmount the desired devices with umount):

cp /usr/lib/systemd/system/tmp.mount /etc/systemd/system/btrfs.mount	(to mount the /tmp device using the btrfs filesystem) 
vim /etc/systemd/system/btrfs.mount	(Under [Unit], remove the line 'ConditionPathIsSymbolicLink=!/tmp' so that it doesn't perform those checks) (Under [Mount], change 'What=tmpfs' to 'What=/dev/sdc1', and 'Where=/tmp' to 'Where=/btrfs', and 'Type=tmpfs' to 'Type=btrfs', and 'Options=mode=1777,strictatime' to 'Options=defaults')
systemctl daemon-reload	(so that systemd is aware of the changes)
systemctl start btrfs.mount	(to mount it)
systemctl status btrfs.mount	(to check the status) (you can change the comment in the btrfs.mount file if you want a more accurate status message to show up instead of 'mounting filesystem')
mount | grep btrfs	(to doublecheck that it's actually mounted)

----

swap is type 82 in fdisk (linux swap)
mswap /dev/sdb6	(to create the swap filesystem on top of that device, after you've already created the partition.
swapon /dev/sdb6	(to activate it manually)
swapoff /dev/sdb6
vim /etc/fstab	(/dev/sdb6	swap	swap	defaults	0 0) (it is 'swap' without a leading slash, because it is a specific kernel interface)
swapon -a	(instead of 'mount -a')

----

Luks ENCRYPTION:

fdisk /dev/sda	(to create a new logical partition, using 'n'...'w' to do so)
partprobe	(to make sure the system is using the new partition)
cat /proc/partitions	(to verify that sdb7 [in this case] exists)
cryptsetup luksFormat /dev/sdb7	(to create the encryption layer, type uppercase 'YES' to acknowledge, then enter a passphrase longer than 8 characters)
xxd /dev/sdb7 | less	(optional, just to see the contents of the device. If encrypted, it will show gibberish)
			(xxd is a hexadecimal view which allows you to see what is physically happening on disk devices)
cryptsetup luksOpen /dev/sdb7 secret	(opens the encrypted device so that it is ready for use. 'secret' is the device's name which we are specifying [creating] here. It will ask for the passphrase to open it, then create a device in /dev/mapper/ )
cd /dev/mapper/
ls	(the name of the device will be shown as 'secret')
mkfs.ext4 /dev/mapper/secret	(to create a filesystem on the encrypted device. Make sure you do NOT try to put a filesystem directly on /dev/sdb7, because it will override the encryption layer and mess things up.)
mount /dev/mapper/secret /mnt
cd /mnt
touch mysecretfile.txt	(now anything that goes here [/dev/mapper/secret/] will be written to the encrypted device)
cd /
umount /mnt	(now we will automate the mounting of the encrypted device. To do this, we need to use two files: /etc/crypttab and /etc/fstab
vim /etc/crypttab	(inside, on one line, type 'secret /dev/sdb7' [without the quotes], which is the name of the device and the device you want to mount)
vim /etc/fstab	( /dev/mapper/secret	/secret	ext4	noauto	0 0 )	['/secret' is just whatever name you want to call it] [we use 'noauto' here because otherwise it could mess up your boot procedure if it tries to mount automatically]

----

SSDs	(fstrim):

fstrim is used because SSDs don't properly clean up after deleting files, so you should use the fstrim command on occaision for maintenance.

/usr/lib/systemd/system/ contains the files fstrim.service (which executes 'fstrim -a' as a oneshot) and fstrim.timer (for you to use if you want to execute it regularly on a schedule).

systemctl enable fstrim.timer	(to make sure that it is enabled)

----

LVM:

gdisk -l /dev/sdc	(gives you a nice listing of partition numbers and type)
			(it's always better to use gdisk when on a gpt partition. fdisk's gpt support is still experimental as of.)

There are 2 different naming schemes you can use: /dev/mapper/vgname-lvname or /dev/vgname/lvname . These are both symlinked to the automatically system-generated /dev/dm-1 (or 2 or 3).

In fstab, doublequotes around the UUID itself are not necessary. It doesn't make a difference whether you use them or not.

lvextend -l +100%FREE -r /dev/vgdata/lvdata	(adds all available disk space from the volumegroup) (-r to automatically resize the filesystem as well, so that you don't make any mistakes in regards to resizing the filesystem)

----

RAID:

Creating a RAID device:

fd00 is the gdisk partition code for RAID paritions (for when you create them). It is not absolutely necessary, but is done to
	prevent errors when the disk is connected to another operating system so that it knows that the disk/partition is a RAID
	volume. If the mdadm utility does not find the proper partition type, it will refuse to create a new RAID array with those
	paritions.
On fdisk, the code is simply 'fd'.

partprobe	(to make sure the system is reading the partition)

mdadm stands for 'multiple device admin'. After you have created the paritions with gdisk, then:

mdadm --create /dev/md0 --level=1 --raid-disks=2 /dev/sdd2 /dev/sde1

Linux generally (but not always) has trouble booting from RAID, so when you mdadm create a new RAID device, it will warn you to
	take extra measures if you want the /boot directory to be stored on the RAID device, such as ensuring that your
	boot-loader understands md/1.x metadata, or using the --metadata=0.90 option.

mkfs.ext4 /dev/md0

mdadm --detail --scan >> /etc/mdadm.conf	(to write the configuration to a conf file)

cat /etc/mdadm.conf

mkdir /raid

mount /dev/md0 /raid

vim /etc/fstab	(put in the line:       /dev/md0	/raid	ext4	defaults	0 0 )(for automount on reboot)

cat /proc/mdstat	(to know if your RAID device is operational)
mdadm --detail /dev/md0	(does the same thing as 'cat /proc/mdstat', but with more info)

---

Recoviring after a disk failure:

First, you can create a hot swap device during the creation of the RAID array by:

mdadm --create /dev/md0 -l 5 -x 1 /dev/sdb /dev/sdc /dev/sdd /dev/sde
	( '-l 5' is RAIDlevel 5, '-x 1' specifies that there is one hot spare, the following are the list of devices that will be
	 used in the RAID array)

*** Maybe VanZight fucked up here by forgetting the 'raid-disks=3' option, i.e. 'mdadm --create /dev/md0 -l 5 -x 1 --raid-disks=3
	/dev/sdb /dev/sdc /dev/sdd /dev/sde' . I'm not completely sure ***

mdadm --fail /dev/md0 /dev/sdb	(to artificially fail the device, which is a conformation to the raid device that the device is
				 generating errors. Normally, you will see a failure in the syslog, then use this command to
				 check it out.)

mdadm --remove /dev/md0 /dev/sdb	(to remove the failing device 'sdb' from the RAID array)

mdadm --add /dev/md0 /dev/sde		(to add a new device 'sde' to the RAID array)

----

NETWORK SERVICES / SERVICE CONFIGURATION:

Most network services are in the LFCE test. For LFCS, all you need to do is basic setup and config.

Managing Web Servers:

Basic configuration of an Apache webserver:

/etc/httpd/ is where the configuration for Apache webserver is, split accross 3 different directories: conf/, conf.d/ and
	conf.modules.d/ .
/etc/httpd/conf/httpd.conf is the main configuration file, with Include, Directory, and DocumentRoot.

		<Directory />
			AllowOverride none
			Require all denied
		</Directory>			(This is the default, safe configuration for low access)

		Include conf.modules.d/*.conf	(default to include modules at config time)
		DocumentRoot "/var/www/html"	(the default for Centos)

		<Directory /var/web>
			AllowOverride none
			# Allow open access:
			Require all granted
		</Directory>			(This relaxes access to content within /var/web . You can further relax access by
						 adding the line 'Options Indexes FollowSymLinks' within this block.)

In the DocumentRoot, Apache looks for a file named 'index.html', whose content will be served whenever a user is connecting to
	Apache.

systemctl start httpd	(to start the Apache webserver)
systemctl enable httpd

elinks http://localhost		('elinks' is a text-based web browser. This line tests connectivity to the localhost webserver)

---

Configuring virtual hosts on Apache:

Virtual Host means that we have a virtual process 'httpd' which serves content for differet webservers, so you need a configuration
	for each of those webservers. So for example, we have two webservers: 'sales.webserver.com' and 'account.webserver.com'. 
	Each of these webservers has its own (seperate) DocumentRoot (webpages) and other configurations. However, there is only
	one process ('httpd') servicing all of these different virtual hosts. In order for find the desired virtual host, the
	client needs to find the server that is running this 'httpd' process. The client typically finds that httpd process by
	going through DNS. So, if DNS entries exist for 'sales.webserver.com' and/or 'account.webserver.com', the request from the
	client will end up with this httpd webserver process. The httpd reads the header of the incoming request, which will
	specify which virtual host the client wants to visit. If the client makes a request that doesn't exist, then he will be
	rerouted to the first alphabetically available virtual host. So, if a client requests 'www.sales.webserver.com', then DNS
	delivers the request to the httpd process, httpd will check its configuration file and see that it is unable to find a 
	virtual host with the name 'www.sales.webserver.com' (because it only has a 'sales.webserver.com' without the 'www.'), so
	it will instead send the client to 'account.webserver.com' because alphabetically 'a' comes before 's'. This is a common
	misconfiguration in Apache. Fortunately in Apache it is easy to configure an alias for these specific cases to prevent it
	from happening.

vim /etc/hosts	(virtual hosts all start with name resolution) (192.168.4.240 sales.example.com, 192.168.4.240 account.example.com)
								(both can point to the same ip address, same ip address as host)
It is good form to create seperate config files in /etc/httpd/conf.d/
vim account.example.com.conf
	<VirtualHost *:80>
	ServerAdmin webmaster@account.example.com
	DocumentRoot /web/account
	ServerName account.example.com
	</VirtualHost>			(This is a very simple virtual host config file)

mkdir /web/account	(cd /web/account)
vim index.html		(just type 'Welcome to Account')

systemctl restart httpd
elinks http://account.webserver.com	(to test the 'Welcome to Account' page)(if you put in the wrong ip address in the
					 /etc/hosts file, this will error out and say 'No route to host')

***NOTE: If you have created virtual hosts, it will disable the normal default contents. So you can do 'rm /etc/httpd/conf.d/account.example.com.conf' and then do 'systemctl restart httpd' to solve this issue.***

----

Configuring FTP services:

2 common ftp solutions on linux are vsftpd and pureftpd. ('vsftpd' = 'very secure ftpd', and 'pureftpd' is very simple)

We will be using vsftpd to create a simple and effective ftp server.

systemctl enable --now vsftpd	(of course after you have already installed the vsftpd package)
systemctl status vsftpd		(to doublecheck that it is indeed activated)

vim /etc/vsftpd/vsftpf.conf	(this file is already existing and populated)
	anonymous_enable=YES, local_enable=YES, write_enable=YES (for users that have permission to write in the first place)
	(These are the defaults)

Install the lftp package (recommended for a real ftp experience)

lftp localhost	(puts you in the localhost ftp server. You can 'ls', 'cd' and 'exit' out.)

grep ftp /etc/passwd	(to look behind the scenes, to find the ftp user and what its home directory is set to [/var/ftp]. This
			 home directory is where contents are provided for other users to access.)

cd /var/ftp/pub, touch file1 file2 file3

lftp localhost, cd pub, ls

get file1	(to download the file from the ftp server to your local user account) 

lcd /root	(to cd into a local directory without having to exit out of the lftp client. It's a convenient command.)

iptables -A INPUT -p tcp --dport 21 -j ACCEPT	(to open the firewall for ftp so that other machines can access it)

---

Configuring a basic DNS server:

/etc/resolv.conf is the DNS resolver on linux, is found in the client's machine. It directs the client to a DNS server, which we
	call the local DNS server for the client. This local DNS server is going to do all the work for the local DNS client. The
	local DNS server first goes the the root domain which in DNS is presented as a dot '.'. It will go to the nameservice of
	the root domain and ask for a list of nameservers of the '.com' domain. The nameservice of the DNS domain will answer to
	the nameserver with a list of names written for the '.com' domain. Next, the DNS server will go to the '.com' domain with
	the request 'RHcert' (the preceding part of the domain), and the '.com' domain will answer back with that list. Once the
	local DNS server knows about it, it is going to fill a cache, so that the next time the client requests that nameserver it
	will not have to go out to the internet to retrieve that information anymore. So the DNS server will then provide the
	client with the answer, and based on that answer the client will be able to send packets out to REcert.com. We will be, in
	this example. building a 'cache-only server'. By using a cache-only server, you make the name lookup process a lot faster,
	and cache-only servers are quite popular because of this speed advantage.

Configuring a forwarding DNS server:

Install the package 'unbound'.

systemctl enable --now unbound

iptables -A INPUT -p udp --dport 53 -j ACCEPT	(DNS listens by default on udp port 53. If the package is too big, it reaches out
						 via tcp, so we use both of them together)
iptables -A INPUT -p tcp --dport 53 -j ACCEPT

vim /etc/unbound/unbound.conf	(there are a couple of parameters here which need to be changed)
	uncomment 'interface 0.0.0.0' so that it can be accessed from outside machines, not just localhost which is default.
	access-control: 192.168.0.0/16 allow	(so that all networks that have an ip address starting with 192.168 will be
						 allowed access [i.e. all localhosts will be allowed access)
	uncomment 'domain-insecure: "example.com"'	(for security if you are going to work with DNS domains which don't have
							 a DNS security key)
	uncomment 'forward-zone:'
		  	'name: "."'	(changed from "example.com" to "." meaning forward EVERYTHING)
			'forward-addr: 8.8.8.8'		(to forward it to the google DNS 8.8.8.8)

systemctl restart unbound
systemctl status unbound	(add an '-l' to show in full)(if an error pops up here, handle it. For example, if it says 
				 'unbound.service failed' then another process/user may be listening on that port [53]. To find it
				 do:)
netstat -tulpen 	(will show that, for example, a 'dnsmasq' process is listening on port 53, so you need to get rid of
			 dnsmasq. Before we kill it, though, we can try to investigate it properly first:)
which dnsmasq	(answers '/sbin/dnsmasq')
apt remove dnsmasq
ps aux | grep dnsmasq
kill 1546 1550
systemctl restart unbound
systemctl status unbound -l	(if it says 'failed address ::1', that is an ipv6 address, so you need to enable ipv6. Many
				 services on modern linux default to ipv6 and just want to do something with it, so disabling it
				 can cause issues.)

-

sysctl net.ipv6.conf.all.disable_ipv6=1 to disable IPv6

and

sysctl net.ipv6.conf.all.disable_ipv6=0 to enable IPv6.

-

Another way to disable IPv6

$ su -
# nano /etc/sysctl.conf

and add these lines to sysctl.conf file

#disable ipv6
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
net.ipv6.conf.eth0.disable_ipv6 = 1

Save sysctl.conf file with new config and run the following command to enable the new settings:

# sysctl -p 

Check your system again

$ cat /proc/sys/net/ipv6/conf/all/disable_ipv6

Now you should see 1 means IPv6 has been disabled on your system.

From http://namhuy.net/1419/disable-ipv6-ubuntu-linux-mint.html

-

---

Configure a basic NFS server:

mkdir /share	(a folder to share)
systemctl status nfs-server
vim /etc/exports	(the configuration file)
	/share	*(rw)	(what you want to share followed by who is allowed access. '*' means all, you can add an ip address or a
			 network address or whatever, and '(rw)' is read/write permissions for everybody except for the root user.)			    If you also want the root user to access it, then do '(rw,no_root_squash)'. By default, the root user on
			 a client is mapped to the user 'nfsnobody'. If you wish to give the root client root access on the server,
			 then 'no_root_squash' gives him that power. This is not a very secure option, but if you want security
			 then you are probably not going to use NFS anyway.)
			(This is a basic minimal config)
			(NFS works on node-based access only. You cannot limit access to specific users, unless you configure a
			 kerberized NFS server which is a lot harder.)
systemctl start nfs-server
systemctl status nfs-server	(to verify that it is active)
showmount -e localhost		(localhost is what you have defined in the /etc/resolv.conf file in this case)
				(will output 'Export list for localhost:' '/share *')(This is proof that the nfs server is working
				 and ready for service.)(Don't forget the firewall to allow clients to access the nfs server.
				 LFCS will accept if you just turn off the firewall completely, because LFCS is not about
				 networking, they just want to give you an impression of what is involved in working with network
				 services.)
---

Mounting on an NFS share persistently: (from the ubuntu client to the centos server)

apt install nfs-common	(to install the showmount utility)
mkdir -p /centos/nfs	(creating a mount point)
vim /etc/hosts
	192.168.4.240	centos.example.com	centos	(to make sure you can access the centos machine by name)
mount centos:/share /centos/nfs		(testing that you can indeed mount if before putting it in /etc/fstab)
	(to doublecheck, use the 'mount' command by itself and look at the last line of output for 'centos:/share on /centos/nfs')
vim /etc/fstab
	centos:/share	/centos/nfs	nfs	_netdev	0 0	('_netdev' option tells the machine that the network is involved)
umount /centos/nfs	(because we will now try out the automount)
mount -a
mount		(we should get the same result as above, with the nfs device showing as the last entry)

---

Configuring a basic samba server:

apt install samba	(the samba server package)
mv /etc/samba/smb.conf.example /etc/samba/smb.conf	(the smb.conf.example file is much bigger and perferred by VanVught)
vim /etc/samba/smb.conf		(which has now been overwritten by the example file as per VanVught's instructions)
	(Being a centos machine, SELinux is involved, so in that case we must set the SELinux context type to samba_share_t by
	 doing 'chcon -t samba_share_t /path/to/directory' later on the command line.)
	(The last block of the example file shows us an example of a share: '[public]''comment = Public Stuff''path = /home/samba'
	 'public = yes''writable = no''printable = no''write list = +staff')
	[share]				(This small block is an example of a simple samba share configuration)
		comment = samba share		
		path = /samba
		public = yes		(allows everybody to share)
mkdir /samba		(the folder to be shared)
semanage fcontext -a -t samba_share_t /samba		(to handle the SELinux context to allow sharing)
restorecon -Rv /samba					(to confirm that samba is set to samba_share_t)

systemctl start smb
systemctl enable smb

smbpasswd -a john	(to create a user samba account, which is like a windows account and not compatible with linux accounts)
			(you will now input the new password. 'john' MUST already be a user that is already accessible on linux,
			 otherwise it is not going to work.)
apt install samba-client	(so that we can interact as a samba user and test the server to make sure it works)
smbclient -L localhost		(will prompt you for root password, but you can just hit enter  and it will allow you to browse
				 functionality because the password is not required for browsing the functionality.)

---

Mounting a samba share persistently (from/on/as the client):

apt install smb-client
smbclient -L centos

mount -o username=john //centos/share /mnt		(for a temporary mount)

apt install cifs-utils		(needed if you get an error saying 'wrong fs type')

mkdir /centos/samba
vim /etc/fstab
	//centos/samba	/centos/samba	cifs	_netdev,username=john,password=password	0 0
		(this is not an elegant method as the username and password are shown in plaintext, but is basic for LFCS. There
		 are other more elegant methods which are more advanced, where you can hide the username/password.)

----

CONFIGURING A DATABASE SERVER: A quick intro to MariaDB

Installing Mariadb database:

apt install mariadb mariadb-server
systemctl start mariadb
systemctl enable mariadb

mysql_secure_installation	(since MariaDB was forked from mysql, it still uses mysql commands)(this script will prompt you)
				(just press enter when it prompts you for root password if this is first time so you don't have
				 an existing mariadb root account)(will ask 'set root password?' [for mariab] say 'y')
				(it's a good idea to say 'y' when asked to remove anonymous users, to make it more secure)
				('Disallow root login remotely?''y' for security)('Remove test databases''y')
				('Reload the privilege tables now?''y' to make the new settings take effect)

-

Creating a Simple Database:

mysql -u root -p	(login as root, '-p' is prompt me for a password. This is going to be the MariaDB root password which we
			 have just set.)
Now we will be in the database, as shown by the 'MariaDB [(none)]>' prompt.
	create database people;		('people' is the name of the database we are creating here)
	use people;		(to put us inside the database so we can start manipulating it. Will say 'Database changed')
				('tables' are the elements in the database that actually contain the data which we want to add)
	create table users(firstname VARCHAR(20), lastname VARCHAR(20), birthyear INT);		(20 is length of character string)
	insert into users(firstname, lastname, birthyear) values("John", "Thompson", "1973");
											(values must be input between "" quotes)
	select * from users;		(to show the data which we have just entered)
	exit	(takes you back out to the shell)

---

***A random example: select * from cities where country = "France";	(to filter down/search specific entries)

----

CONFIGURE BASIC EMAIL HANDLING: 

A null client is a linux server that is running an SMTP process (which are used to send mail to the outside). Null clients are typically configured with a forwarder, which is an email server which sits somewhere else and is capable of sending emails to the rest of the world. We can call this an SMTP server. An IMAP or POP server (which is also running an SMTP process) is what allows a user to receive incoming emails. It listens for incoming messages on port 25 across all IP addresses (0.0.0.0:25). So typically null clients are just for sending messages outside to the internet, NOT receiving messages. 

Configuring a null client: Configuring postfix for email delivery.

systemctl status postfix	(on centos, postfix is started automatically because it is needed by processes which have to send
				 information out on the internet. The automatically running configuration is a mail client config.)
vim /etc/postfix/main.cf	(contains a lot of parameters which determine what the postfix mail server is going to be doing.)
	inet_interfaces = localhost	(means that your postfix mailserver won't accept any incoming messages. You can see this
					 by doing 'netstat -tulpen | less' which shows '127.0.0.1:25' which is localhost.)
	relayhost = [192.168.4.200]	(relayhost specifies the default host to send mail to when no entry is matched in the 
					 optional transport table. It is usually put in [] square brackets to avoid doing a DNS MX
					 mailserver lookup and just send it directly to the relay host. You should make sure that
					 you talked to the owner of the relay host mailserver because by default mailservers will
					 not accept being used as relay hosts. It may work if you have set up DNS and the MX
					 record for your DNS domain properly. But if you are just setting up a simple linux
					 mailserver then you will not be able to send messages out because it will probably fail
					 due to the relay host's mail verification. So make sure you do the appropriate config on
					 the relay host as well.)
systemctl restart postfix	(always need to do it when you change the configs, because postfix doesn't check the config file
				 to see if any changes have been applied.)

-

Configuring postfix for local email reception:

vim /etc/postfix/main.cf	(we will now be looking at parameters required for receiving emails)
	inet_interfaces = all	(to have it listen on all internet interfaces. Change it from 'localhost' to 'all')
	myorigin = $mydomain	(indicates where the message seems to be coming from. '$myhostname' will include the sending user's
				 hostname in the email's 'from' field [john@server1.example.com]. To remove that and only have the
				 domain show, set it to '$mydomain'. In email handling, it's common to not include the $myhostname
				 in the origin parameter.)
	relayhost = [198.168.4.200]	(same as the previous, for forwarding emails)
	mynetworks = 192.168.4.0/24	(a space-seperated list of networks that are allowed to relay. 192.168.4.0/24 means that
					 only users on the local network are allowed to relay messages.)
	inet_protocol = ipv4	(the internet protocol used by this mailserver. 'all' means it will use ipv6 as well as ipv4,
				 which could cause some problems because by default it will bind to ipv6 and won't work if your
				 ipv6 isn't completely configured and working properly. If you don't have that, change this
				 parameter to 'ipv4'.)
systemctl restart postfix

----

CONFIGURING A WEB PROXY: 

Web proxies can be used to speed up traffic to webservers, and to increase security for users on the local network. It is important that if a web proxy is set up for the users, that it be the only way that the users are allowed to access the internet so that
 they can't just simply go around it and connect directly to the internet.
Web proxies can build up a cache to access website faster after the first visit to them. However, this isn't widely used anymore as if the link between the proxy and the internet is slow then it defeats the purpose, so that is the main reason that web proxies
 aren't commonly used anymore.
Web proxies can also be used for content filtering, and this is the most important reason that they are used today. You can create access control lists in a web proxy to prevent your users from visiting certain sites. This is why many companies like to used web proxies as a security measure to make sure that employees only go to websites that are relevant to their work.
A web proxy only applies to web traffic, meaning http traffic. You can sometimes configure them for ftp traffic, but other traffic is not dealt with by web proxies and will typically be denied. 

Configuring a basic squid proxy: (It will automatically be configured as a web cache, you do not need to change any configs.)

apt install squid
systemctl status squid	(just to verify that a systemd service file for squid is available)

vim /etc/squid/squid.conf	(contains examples of how you can configure an access control list. It's already set to use all the
				 private addresses which have been defined in the system. Aside from that there is not much.)
systemctl start squid		(to start the squid web proxy. No changes to the config file were needed in this case.)
systemctl status squid		(to make sure it is operational)

Now we need to configure our browser to use the proxy:

Firefox > Preferences > Advanced > Network > Connection Settings
	Here it has 'Use system proxy settings' already selected, however you may need to instead select 'Manual proxy
	 configuration' and type in under 'HTTP Proxy' '192.168.4.240' (the ip address of the local proxy) and Port '3128'.
	 (you can find the port by 'netstat -tulpen | less' where the name 'squid' will be there under the PID/Program field.)
Now, try to connect to a website in Firefox to make sure that you can still do that with the squid proxy in place.

----

MANAGING VIRTUALIZATION: 

Hypervisor-based solutions (KVM and Xen) use linux as a virtualization server, so you won't be doing anything else on that machine.
Workstation-based solutions (Virtualbox amd VMware workstation/player) do not perform as well as the hypervisor-based solutions.

Creating a KVM virtual machine:

Advanced Options > make sure you tick the box next to 'Enable hypervisor applications in this virtual machine'. This will make sure that you can create embedded virtual machines (which is a virtual machine within your virtual machine, which of course only makes sense in a test environment).

yum groups list	(to make sure that virtualization software is installed) ***check for apt equivalent***
	***in ubuntu/debian, this is 'tasksel --list-task ***
	Will show that the option 'Virtualization Host' exists.
yum groups install "Virtualization Host"	(the "" quotes are used because there is a space in the name)
	***tasksel does not show virtualization in ubuntu Goddammit. Need to find how to do this shit on ubuntu***

systemctl start libvirtd	(libvirt is the supporting service which we need)
systemctl status libvirtd	(to verify)

yum provides */virt-manager	(to find which additional package we need to install)

yum install virt-manager	(is also present in ubuntu)

It has a graphical user interface (Virtual Machine Manager), click Create a New Virtual Machine, select Local Install Media (iso).
(make sure libvirtd is running otherwise it won't work). 512 MB RAM and 4 GB hard disk space image is enough. You do not have to
care about 'Network Selection' because it will automatically select the virtual network by default. 

-

Managing KVM virtual machines:

virsh --help	(virsh is a very rich command which you can use to manage all aspects of virtual machines)
virsh --help | less	(there are over a hundred different options that you can use)

virsh list	(shows which VMs are currently available, and their state [i.e. 'running'])
		(if you have VMs which are not currently running, 'virsh list' is NOT going to show them)
virsh list --all	(to show ALL VMs EVEN if they are NOT running)

virsh start centos7.0	('visrsh start <Name of the VM>')
virsh stop centos7.0
virsh autostart centos7.0	(makes sure that anytime the host machine is restarted, the VM is automatically started)
				(marks as autostart)
You can use 'virsh' or the 'Virtual Machine Manager' Graphical Interface to do the same things. The 'lightbulb' button shows and
	lets you manage the hardware that is currently used by the virtual machine. From the dropdown menus you can easily clone,
	shutdown, migrate, pause and delete VMs.

-------------------- FINISH LFCS Sander VanVught ---------------------^^^--------------------------------

Mastering grep: theurbanpenguin 1 hour webinar:

grep -F 127.0.0.1 /etc/hosts	(-F parses only regular characters [pure text] WITHOUT meta characters, so the '.'s are actually 
				 periods and NOT the metacharacter for ANY SINGLE character. Otherwise, it would also match 
				 '127a0a0a1'. People call the '-F' option 'fast grep' or 'fixed grep'.)

'fgrep' is just a shell script, an alias which calls 'grep -F'.

grep 'raw characters without the bash shell expanding metacharacters its own way'	(the use for single quotes)	
	We don't use "" double quotes because doublequotes still allow things like the dollar sign $ to be expanded by the shell.

grep -v '^#' /etc/services	(displays only uncommented lines, i.e. lines that don't begin with a '#'. However, it also matches
				 blank [empty] lines. To avoid this, we can do the next example:)

grep -ve '^#' -ve '#$' /etc/services	(same as above, but removes the empty lines)(-e specifies each pattern seperately, for 
					 multiple patterns. It will match all patterns seperately i.e. any line which contains any
					 one of the patterns. Without it, grep will confuse a pattern for a filename.)

grep -E		(includes EXTENDED or ENHANCED regular expressions.)

grep -Ev '^(#|$)' /etc/services		(a more advanced way of doing the above example. The '-E' option activates the '|' [OR])

'.' is any single character, '?' refers to the previous character and can make it optional. For example:

grep -E 'colou?r' file.txt 	(Will return both 'color' AND 'colour' if both are found in the file. Thus, the 'u' is made
				 optional: either zero or one occurences is accepted. The '-E' is required to activate the '?')

grep -w	'the' file.txt		(Looking ONLY for the word 'the'. Will NOT match 'then', 'them', 'their', 'there' or 'they'.)

grep -w 'root' /etc/passwd	(Will match 'root:x:0:0:root:/sbin/nologin' because 'root' is a word seperated by non-letters.)
				(trying -w with non-letters may be unreliable because of word seperating logic)

\grep -w 'the' file.txt		(escaping the grep allows you to run it without using the 'grep --color=auto' alias)

grep -o 'root' /etc/passwd	(shows ONLY the match itself, without printing the context of the rest of the line. Useful if you
				 want to only extract a word from the configuration as often as it shows up, rather than the whole
				 line.)

grep -i 'permit' /etc/ssh/sshd_config	('-i' for case insensitive)

grep -i -A5 'permit' /etc/ssh/sshd_config	(show the matched line AND the next 5 lines after it)
						('-A' = After, '-B' = Before, '-C' = Context [lines before and after])
grep -i -C5 'permitroot' /etc/ssh/sshd_config	(to see the context of 'PermitRootLogin in PAM)

grep -E '[0-9]{5,}' /etc/services	(show ports which have a minimum of 5 digits [no maximum limit for the digits])

grep -E ' [0-9]{5,}/tcp' /etc/services	(same as above, but show only tcp ports which match the 5 digit+ criterion)
					('{3,3}' is 'minimum of 3 digits, maximum of 3 digits'. Same as just {3})
					(The blank space in the beginning was included to get rid of false matches.)
	'\s[0-9]{3}/tcp'		('\s' instead of a ' ' space would have been better, since that would account for ALL
					 KINDS of whitespace, including tabs, not just spaces.)

grep -E '^\s*#' file.txt	(matches line that begins with a # comment, regardless of whether the comment has any number of
				 spaces preceding it or not. The * means optional [any number including zero] and requires the -E)

cat -vet file.txt		(display all text including non-printable and whitespace characters i.e. '^I' for 'indent')

grep -iE '^a.*1$' /usr/share/doc/wamerican	(matches 'A-1','A1','a1', and 'ASN1')

grep -i run-parts /etc/cron.d/*

+ means {1,} i.e. one with no maximum limit. Of course it is an extended regular expression -E.

grep -E 'ocle$' /usr/share/doc/wamerican		(will match all words ending in 'ocle')

-----------------------------------------------------

sed	(ULI101 lecture Eric Brauer yt):

sed '3,6 p' file.txt	(prints ALL LINES, AND lines 3 through 6 [so lines 3-6 are printed twice, while the rest are being printed)

sed -n '3,6 p' file.txt	(ONLY prints lines 3-6. '-n' suppresses the default output which is to print the entire file once.)

sed '5 d' file.txt	(delete the fifth line, i.e. only print the rest of the lines) ( 5,8 for a range, like above)

sed '5 q' file.txt	(prints ONLY the first five lines. 'q' is for 'quit'. i.e. quit the operation once the default printing is
			 done up to line 5. i.e. quit ON the fifth line is a better way to say it. This is the same functionality
			 as you would get with the 'head -5' command.)

sed -n '/chevy/ p' cars.txt	(Anything between the // is a regular expression. This searches/matches lines containing 'chevy'.)
sed -n '/chevy/I p' cars.txt	(the 'I' at the end of the search expression tells it to ignore case, so 'Chevy' is also matched.
				 /[Cc]hevy/ would achieve the same result for the first letter only.)
sed '/^[fv]/ d' cars.txt	(only print lines which do NOT begin with an 'f' or 'v')

sed '/ [0-9][0-9][0-9]$/ q' car	(start printing the whole file, but quit once you reach a three digit number at the end of a line.)

sed 's/oldpattern/newpattern/' cars.txt	(will change the first occurence of oldpattern on every line)('/g' to allow for multiple
					 changes per line.)

sed 's/.*Disconnected from//' ssh.txt	(delected anything before and including 'Disconnected from' from that file, to remove
					 usless clutter.)
					('.*' can be very greedy, matching as many times as possible, so you may want to try
					 another way of matching. '.*?' makes if non-greedy. '+' is also greedy, so '+?' will make
					 it non-greedy, meaning it will not try to match as much as possible. Apparently, the '?'
					 makes the expression stop at the first math and not go on forever. In other words,
					 the ? means that once you hit the Disconnected keyword, you start parsing the REST of the
					 pattern. This way, repetition of the word 'Disconnected' [for example. if a hacker tries
					 to trick the log by making his username itself 'Disconnected', that will not confuse you
					 if you took appropriate precautions in your regular expression, as follows:)
					(Regular expressions, by default, only match per-line anyway. They do not match across
					 new lines. Since sed operates per-line, it will repeat the regular expression for each
					 line.)

sed s/oldpattern/newpattern/g oldfile.txt > newfileoutputfile.txt

echo 'Tuesday' | sed s/Tues/Mon

The special character '&' in the replacement string stores the FOUND (matched/returned) string. So:

sed 's/[0-9]/&&/g' file.txt 	(will result in 11, 22, 99... basically whatever was matched [1,2,9], will be regurgitated using 
				 the '&' special character. So this is a way to convert a single digit to matching double digits.)

sed 's_\([0-9]\)\([0-9]\)_\2\1_g' file.txt	(will reverse the digits of 2 digit numbers [12 becomes 21, 2007 becomes 0270]) 

sed 's/[0-9]/WORD/2g' file.txt	(replaces starting from the 2nd digit if present. It is called 'sed pattern flags', where the 
				 number before the 'g' at the end determines which digit/character the replacing starts at. So
				 '13' would become '1WORD'.)
		('/p' at the end will print the newly replaced line, '/w newfile.txt' writes it to the newfile.txt, '/I' is ignore
		 case. Note: you can combine flags, but '/w' must come last. You can pipeline sed commands. And don't use more than
		 one '/')

sed -f file.txt		(load sed commands from a file.txt)

sed '1,100 s/a/A/'	(to create ranges [ a.k.a line restrictions], apply the command to the first 100 lines. Line restrictions
			 can be inversed with the '!' exclamation mark.)

:%s/old/new/g	(this is how you use it within VIM)(':' means command mode in VIM)
:%s/old/new/gc	(the 'c' tells VIM to prompt you for confirmation for each replacement)

regex101.com is a nice regular expression debugger, useful if you want to know exatly what a long regular expression does and are having trouble figuring it out.

Anchors: If '^' is not placed at the beginning of the pattern and/or '$' is not placed at the end, then they will not act like an
	anchor. This is true for both sed and grep.

grep -n ^'work' file.txt	(shows all lines beginning with 'work')('work'$ for end of line)
	(both '^work' and ^'work' seem to give the same result)(-n shows the line number)
grep -n ^'[ab]' file.txt	(lines that start with either 'a' or 'b'. The [] denotes a single character, so a or b.)

Modifiers (* ? +) need to be placed AFTER a character set.  

grep -n '[0-9]\{2,4\}' file.txt		(shows lines which have 2-4 consecutive digits)

grep -n '\<work\>' file.txt	(will return the exact word, so 'network' or 'working' will not be matched)

Backreferencing a.k.a remembering patterns with \(,\) and \1. \1 can use 1-9: 1 is the first backreferenced group() on the line, 2
	is the second, and so on. Backreferencing remembers the actual resultant value that was matched to each specified pattern.
	So \([a-z]\)\(a-z\)[a-z]\2\1 would produce a palindrome such as 'radar', because the last letter would be a repeat of the
	first, and the 2nd to last letter would be a repeat of the second, while the third/middle letter would be any letter.
	Backreferencing works in both grep and sed.

grep -n '\([0-9]\)\1' file.txt 	(prints all lines which contain 2 consecutive equal numbers [11, 22, 99])

grep -e '-i' file.txt	('-e' is the escape parameter, so you can search for literally '-i' without interpretation)



---------

awk '$1 == 1 && $2 ~ /^c.*e$/ {print 0}'	(first field = 1, 2nd field starts with a 'c' AND ends with an 'e', print the whole
						 line.)
Awk is great for operating on multiple columns.

In awk, 'BEGIN' is a special pattern which only matches the zeroeth line, 'END' matches the line after the last line.

awk 'BEGIN { rows = 0 } $1 == 1 && $2 ~ /^c.*e$/ {rows += 1} END { print rows }'
	(This basically accomplishes the same thing as '| wc -l'. In the beginning, you set a variable 'row' to equal zero, then
	 after each match of the expression(s) you add one to that variable, and print the final value of the counted variable.)

awk '$1 != 1 { print $1 }	(print the first field whenever the first field does not equal the number '1')

paste -sd+	(will stick the inputs together, seperating them with a '+' symbol [1+2+3+4]. Then, you can pipe it to 'bc -l'.)

In awk, '==' will only accept EXACT match, whereas '~' will match any even a longer field just CONTAINING the pattern (Ali > Alice)
	'!~' matches any line which does NOT contain the specified pattern in the specified field.

awk '(NR>=0 && NR<=11){print} (NR==11){exit}' file.txt		(prints the first ten lines of file.txt) (Same functionality as
								 'head file.txt' or 'sed 11q file.txt') ('NR = Number of Records')
awk 'tolower($0) ~ /dir' file.txt	(to perform a case-insensitive search, like 'grep -i'. The output will still display
					 normally as original, NOT all lowercased.)

Awk basically treats a file as if it were a database table, so it sees in fields.

Investigate whether NR % == 1 denotes even numbered lines in awk. Could come in handy.



----

echo "1 + 2" | bc -l	(will output '3')('-l' a.k.a '--mathlib' defines the standard math library)

rustup toolchain list | grep nightly | grep -v 'nightly-x86' | grep 2019 | sed 's/-x86.*//' | xargs rustup toolchain uninstall
	(list the files, find the old files, remove the differences in file names [date suffixes], uninstall each one)

It is common to use a dash '-' to mean input and/or output in the context of bash commands.

convert - -colorspace gray -	(the first dash means input that is piped in [stdin], the last dash means stdout)

Many commandline programs can be used for all data, not only text. For example, 'cat /dev/video0 | ssh <aremotemachine>' will pipe
	streaming video from your machine over ssh to the specified remote machine, whose user can pipe it into a media player to
	watch it in real time.



-------------------------------------

sort -nk1,1	(sort by numerical value, starting at the first column and stopping at the first column [i.e. sort only by the 
		 first column])

sort file.txt | uniq	(since uniq ONLY detects duplicates that are right next to each other, you HAVE TO run sort FIRST,
			 otherwise uniq won't do its job.)

-------------

printf "%32s %s\n" "File name" "File Type"	('%32s' is 32 characters wide, right-justified. '%-32s' would be left-justified.
						 The first format [%32s] acts on the first field [File name], the 2nd [%s\n] acts
						 on the second field [File Type]. The output of this is:)
		      File name File Type	('File name' is right-justified and 32 characters wide, File Type has a newline at
		      				 the end of it.)
File name		       File Type	(The result of it was left-justified '%-32s'.)
	(%s is shorthand for %0s . It just automatically pushes it to the left of available space and uses the minimum amount of
	 characters by default.)



----START LinuxAcademy LFCS-------------------------(kinda old and baddish, ubuntu 14.04)--------------------vvv-------------

du -sch		(usage: summary and total, human readable. '-sh' without grand total, just the summary)

du -sch /home/*	(to show all the individual subdirectories, can use just * for current directory)

sort -h	(sort by human-readable values [G,M,K])

sort | uniq -w 6	(check only the first 6 characters of each line)(-c to display the count [number of occurences])

cat syslog | uniq -c -w 6 | sort	(sort by number of occurences)

fmt -u file.txt > newfile.txt	(format the file to have unform spacing. If no redirect, then it just goes to STDOUT.)

cut -d ';' -f1 file.txt		(take the first field of each line, delimited by a ;)

cut -d ';' -f1-2 file.txt	(take the first 2 fields of each line, and keep the first delimiter [between the first and second
				 fields] on each line with them.)

tar -cvf newarchivetobecreated.tar filesandfoldersyouwanttoarchive	(no compression, no permission preservation)

tar -tvf archive.tar | less	(lists the contents of an archive)(can '| grep pattern' instead of less if you need)

gzip archive.tar	(will create a 'archive.tar.gz' file) ('tar cvzf archive.tar.gz filesandfolders' for the same result)

tar zxvf archive.tar.gz		(to uncompress and extract files/folders into the current directory)

tar cvzf archive.tar.gz --exclude=file1.txt datadir/	(exlude a file from the arhive)('p' to preserve permissions)

-

(VanVught's presentation was much better, refer back to that. This is just a quick review.)
pvcreate/pvremove, pvdisplay, vgcreate myvolumes /dev/sdb1 /dev/sdb2, vgdisplay/vgscan, vgrename myvolumes mydatavolumes,
lvcreate --name firstgroup --size 12G mydatavolumes, lvdisplay/lvscan, mkfs -t ext3 /dev/mydatavolumes/firstgroup,
mount -t ext3 /dev/mydatavolumes/firstgroup /mnt/tmp, df -h, vgdisplay, fdisk -l, umount tmp, vgdisplay, 
vgextend mydatavolumes /dev/sdd1 (assuming that I already formatted it as an LVM volume), vgdisplay,
lvextend /dev/mydatavolumes/firstgroup /dev/sdd1, lvdisplay, e2fsck -f /dev/mydatavolumes/firstgroup,
resize2fs /dev/mydatavolumes/firstgroup, mount -t ext3 /dev/mydatavolumes/firstgroup tmp, df -h

-

df -h, cat /proc/swaps,fdisk /dev/sda3, mkswap /dev/sda3, swapon -v /dev/sda3, ls -al /dev/disk/by-uuid/ (copy that into fstab)

-

File Attributes:

ls -al /usr/bin/passwd	(shows that it has the setuserid bit set, SUID)(any user can run it, but only the root user can specify
			 an account. So 'passwd root' will not work unless you are already root.)

chmod g+s sales/	(to set the GUID on the sales directory)('chmod 2755 sales/' does the same thing.)

chattr +i file1		(make file1 immutable, so you cannot delete it)
chattr -i file1		(go back it it being normal, so that you can delete it)
chattr +a file1		(make the file append-only, so you cannot overwrite it with '>', only append with '>>')
chattr =a file1		(remove all other attributes except for +a, and make it +a)

-

find /etc -not -iname "test.txt"	(shows all files in /etc which are NOT 'test.txt')

find / -type c 	(shows all the character devices in the system)('-type l' is symbolic links, '-type d' is directories', 'f' files)

find / -type f -name "*.log"	(find all the '.log' files on the system)

find /usr/bin -size +27000c	(find all files larger than 27000 bytes	[c = bytes])

find / -mtime 1		(files with a modification time of one day ago or greater)

find / -mtime -1	(files that were modified less than a day ago)('-atime' = access time)

find / -user syslog	(all files which are owned by the syslog user)

find / -perm 755	(all files with permission of 755)

find / -name "test.txt" -exec chmod 700 {} \;

which ifconfig		(if you know the name of a file and that it might be in your $PATH, then you can use which to find it)
			(outputs '/sbin/ifconfig')

sudo updatedb	(so that you can use locate), locate test.txt

-

Mounting Networked Filesystems:

192.168.1.150:/NFS_SHARE /mnt/tmp nfs defaults 0 0	(for compatibility reasons we use the ip address because sometimes the
							 network will start before the DNS service, so you don't want the mount to
							 fail because it can't see the name of the system connected to the ip.)

apt install samba-common-bin cifs-utils		(to be able to use samba networking)

smbclient -U john -L share	(lists all available shares on that system, will prompt for your password first.)

echo "username=john" > /mnt/.smbcredentials	('.smbcredentials' is the standard naming convention)
echo "password=12345678" >> /mnt/.smbcredentials
chmod 600 /mnt/.smbcredentials		(to make sure noone else can read/write to that file)
vim /etc/fstab
	//192.168.1.150/news	/mnt/tmp	cifs	credentials=/mnt/.smbcredentials,defaults	0 0
(cifs is a standard networkshare filesystem, will always be used for samba shares)
(the credentials file can be anywhere you want, as long as you have permissions and it actually exists where you are pointing)
mount -a
df -h	(to double-check)
cd /mnt/tmp	(where you will see all the files/directories from the network share stored)

-

Troubleshooting filesystem issues:

fsck /dev/sdb1	(perform a quick check)(MUST BE UNMOUNTED FIRST, otherwise it will give you an error)
fsck -a /dev/sdb1	(automatically clean the filesystem)
echo $?		(will echo the last error code that the application has given you. '0' is no errors, 1 = fs error corrected, 2 = 
		 need to reboot, 4 = errors have been left uncorrected, 8 = there's been an operational error, 16 = syntax error,
		 32 = user request is cancelled [fsck], 128 = shared library error [something to do with the utility itself])
Since it needs to be unmounted, if you have to run fsck on a root partition they you will probably boot from a rescue disk first.

-

Creating backups (with dd):

dd if=/dev/sdb1 of=/root/sdb1.img	(will not give you any feedback while it's doing it, only after it completes)
ls -al /root/*.img

dd if=/dev/sd0 of=/root/vmtools.iso	(you can easily mount an .iso file into another directory, because the .iso is a well-known
					 utility for being able to mount using the loopback filesystem)

-

Creating Local User Groups:

cat /etc/group

addgroup test1	
groupadd test2		(will both automatically be added to the /etc/group file with new GIDs)

newgrp test1	(to switch your current group to 'test1' so that you can create files/directories with the groupowner 'test1', but
		 you will be prompted for the password each time you switch to a new group, so you have to go and set the group 
		 password first:)

gpasswd test1	(will prompt you to set the new password)

chgrp test2 test2.txt	(will change the group owner of test2.txt to test2, as long as you are already a member of the test2 group)

-

Managing Local User Accounts:

useradd -d /home/testuser testuser	(the old, manual way)
cat /etc/group	(the last entry should be 'testuser')
passwd testuser	(we need to create the password for that user, it will prompt us to type and retype it)
mkdir -p /home/testuser
chown testuser:testuser /home/testuser	(user:group)
exit
su - testuser	(to switch to the testuser account)
pwd	(will show '/home/testuser')
Now, copy the files in the /etc/skel directory into the user's home directory.

adduser testuser1	(new automated way, will automatically add user, group, user to group, create home directory, copy files
			 from /etc/skel, then prompt you for each field starting with the password)
userdel testuser1	(to delete the user and remove everything [UID,GID] except for the user's home directory, which you will
			 need to manually remove with 'rm -rf /home/testuser1')
userdel -r testuser1	(will automatically also remove the user's home directory)

-

Managing User Accounts:

cat /etc/passwd
cat /etc/shadow

vim /etc/bash.bashrc	(system-wide bashrc which becomes the bashrc for any newly created users. So it's a good place to put
			 scripts that you want to be run for each user at startup.)
/etc/skel/	(contains files that will automatically be initialized for new users at login. For example, in /etc/skel/.profile
	   	 you can set the EDITOR environment variable to vim. However, new files put in here or new changes made are NOT
		 automatically pushed to existing users, thus you risk 'clobbering' existing users' configurations. This is why it
		 is best to keep this directory small. It's better to put global configs into global files such as /etc/profile ,
		 which will reset anytime anybody logs in. Then users will make their own config files in their home directory to
		 make their personally desired changes. 

If you manually change the UID/GID by editing the /etc/passwd file, you need to reset the home directory as the root user so that
	it matches the new UID.

vipw	(opens the password file for editing [as root]) (add a new line at the end to manually add a new user)(Then:)
vigr	(testuser:x:1002: to the end of the file, matching the user you created in vipw)
mkdir -p /home/testuser		(do all this as root, so it is currently owned by root)
cd /home/testuser
cp /etc/skel/.* .
cd /home
chown -R testuser.testuser testuser/
ls -l	(to doublecheck that the /home/testuser/ directory is owned by testuser:testuser)
passwd testuser		(to finally set a password for testuser)
exit
su - testuser	(will prompt you for testuser's password, and put you in testuser's home directory as testuser)

-

Managing User Account Attributes/Properties:

chfn	(change full name field of the user who you are currently logged in as, will prompt you for user password)

cd /bin, ls -al *sh	(to find available shells)
chsh	(change shell), passwd	(change password)
chsh <username>, /bin/false	(prevents that user from logging in normally or via ssh)
		 /usr/sbin/nologin	(prevents that user from logging in, even if they are already logged in and the system/user
		 			 hasn't reset itself yet. Stronger than /bin/false, which doesn't prevent a user from doing
					 'su - testuser' to get around this restriction. With nologin, nobody will be able to log
					 in as that user [user is unavailable].)

-

Managing User Processes:

top: 20 is the LOWEST priority (PR) on a system, -20 is the HIGHEST priority on a system.

htop is a little more friendly-looking.

ps aux	(shows all processes run by all users. The ps command is more powerful/flexible than top/htop.)
ps axjf	(gives a more formatted tree view)

pgrep bash	(similar to 'ps aux | grep bash')

kill bash	(you will need to be the owner of the process or root in order to kill a process)

renice 10 1929	(change priority from 0 to 10)
nice -n 20 /bin/bash	(start a bash process at the lowest priority)

-

Restoring Backed Up Data:

dd if=/root/sdb1.img of=/mnt/diskc1	(WILL NOT WORK! YOU NEED TO WRITE DIRECTLY ONTO THE PARTITION, NOT ONTO THE MOUNT POINT.)

dd if=/root/sdb1.img of=/dev/sdc1	(This will work.) (Remember, the dd command will not show progress, only feedback after
					 		   it fully completes.)
Remember to UNMOUNT the filesystem FIRST, before doing the above.

The above was to rebuild the actual .img file. If you just want to mount the image itself, you will only be accessing it in read
	mode, because in order to update that .img file you would need to recreate like we have done above.)

mount -o loop /root/vmtools.iso /mnt/tmp	(to mount the image file in loopback [readonly] mode)(will show up in 'df -h')

-

su -	(the '-' carries forward all the environment variables of the specified user to switch to, in this case root)

When you ssh in, the admin will usually disable external login as root so that the root password cannot be brute-forced from outside.

visudo will check that you haven't done anything to prevent ALL USERS from logging in.

Inside the sudoers file, you can add a line: 'john ALL=(ALL:ALL) ALL' to give john elevated privileges using his own password, so
	NO NEED for the root's password. Then, he can:
sudo su -	(which allows him to become root without knowing the root user's actual password, just his own password. This way,
		 the admin doesn't have to give the root password to other people while stil allowing them to work as root.)
		(You can also use this method to reset the root's password by using the command 'passwd' when you are acting as
		 root [actually being elevated john, not root].)

Another method is to add a user to a group that has the desired priviliges, by:
vim /etc/group
	(Change the line 'sudo:x:27:ubuntu' to 'sudo:x:27:ubuntu,john')
Then, you can do the same 'sudo su -' maneuver.

-

export PATH=$PATH:/root/scripts
env | grep PATH		(to double check)

-

Conditionals:

#!/bin/bash
DIRECTORY="/root/scripts/test"
if [ -d "$DIRECTORY" ]; then
	echo "This directory exists"
else
	echo "This directory does not exist"
fi

for count in 1 2 3 4 5		(In 'for' loops, you know the number of times it's going to execute. In 'while' loops, you don't.)
do
	echo "This is line number $count"
done

while read HOST; do		(will execute the ping command for each value [ip address] listed within the 'myhosts' file)
	ping -c 3 $HOST
done < myhosts		(read in the 'myhosts' file and use it to populate the 'HOST' variable)

-

#!/bin/bash
DIRECTORY="test"
if [ -d "$DIRECTORY" ]; then
	for count in 1 2 3
	do
		echo "Directory exists - Line # $count"
	done
else
	while read HOST; do
		ping -c 1 $HOST
	done < myhosts
fi

---

ssh user@54.175.214.244		(logging in to an LFCS server)

su -	(will prompt you to immediately change your password)

-

dpkg -l (lists packages, will match to the name of the next argument: 'dpkg -l xauth')

dpkg -L xauth	(lists all files that were installed to the system from the xauth package)

apt show	(more details than 'apt search')

apt-cache stats	(shows total number of packages, and categorizes them by many states)

Be careful when you use wildcards (*) to install packages; you will usually end up with more than you wanted and create broken packages.

-----FINISHED LFCS Linux Academy ---------------------------------^^^----------------------------------------------------------


-----START AGAIN Sander VanVught LFCSA----------------------vvv----------------------------------------------------------------

Freevidsyt:

chvt 5	(to go to virtual terminal number 5)(number 7 on ubuntu is the GUI)

The 'chvt' command is useful when working in Virtual Machines, because with Ctrl and Alt it might mess with the 'capture keyboard'
	settings and things could get very finnicky.

vim /etc/securetty	(A list of all consoles which are considered secure. If you remove an entry from this list, then the root  
			 user will no longer be able to access that console. It will say 'Login incorrect' even if you type in the
			 correct password. Once the root user is locked out like that, you can get around it by logging in to that
			 console as a normal user, then do 'su -' to login to a root shell, then 'chvt 7' out of there.)
			(The original purpose of this file is to include all local tty's and remove all network tty's which are by
			 their nature considered unreliable.)

'su' alone does not set the environment variables completely correctly, so it is recommended to use 'su -' instead. 'su -' gives
	you a LOGIN SHELL, whereas 'su' only gives you a SUB SHELL.

su - john	(to switch to user john, in a login shell so that you also get all of his environment variables loaded in. Useful
		 for testing certain functionalities as that user.)

usermod -aG wheel john	(to add john to the wheel group, so that he has access to do sudo tasks)
			(For this to take effect, john must logout and log back in to the system, because during login is when
			 the user gathers his credentials for the system from the environment variables in which they are stored.
			 So even if commands like 'id' and 'sudo -i' show that the change has taken effect, it wont work until he
			 logs out and back in.)

Ubuntu was designed for you to use 'sudo' instead of root. However, because this is a lot of typing, you can (against popular recommendation, though VanVught says it's fine) do 'sudo su -' or 'sudo -i' (to open an interactive shell, the '-i' is for 'interactive'). 

When you use the 'w' command to list active users/sessions, 'pts' means 'pseudoterminal session', usually meaning a remote session
	where someone has logged in over the network, as opposed to local 'tty' sessions.

---

Lesson 1:

There is a slight difference between typing 'exit' and 'logout' in the console. 'exit' is just quitting the shell, while 'logout'
	is really logging out the user. Most of the time, they lead to the same result, but VanVught will later explain the
	difference between them. 

The Seven Essential Commands: whoami, hostname (tells you which computer you're working on), date (set date & time), uname (to get
	more information about your system and what it's doing), passwd (change passwords), touch, last (list of users which have
	recently logged in)

-

man pages: Sections: 1. User Commands 4. Devices 5. Config Files 8. Sysadmin Commands

man 8 useradd	(will request entries only from section 8)

man -a useradd	(to show you matches from EVERY section, if you are not sure which one you need. You will see the item from the
		 first available section, then when you quit out of that page [by pressing 'q'] it will bring up the page from the
		 next available section.)

In the manpages, when describing command syntax, [] means optional, while {} means required, and the | between each thing means you
	chose one or the other.




15



